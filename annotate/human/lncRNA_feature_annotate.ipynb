{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate multi-omics features for lncRNA genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 Get genomic position for lncRNA genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step1.1 Get position from BED files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Results saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Set the directory containing ensembl data\n",
    "ensembl_dir = \"../../reference_lncRNA/human/bed/ensembl/\"\n",
    "\n",
    "# Read lncRNA ID and gene_name information\n",
    "lncRNA = pd.read_csv('../../data/LPI/human/lncRNA.csv')\n",
    "\n",
    "# Read NONCODE BED files\n",
    "noncodev5_bed = pd.read_csv('../../reference_lncRNA/human/bed/NONCODEv5_hg38.lncRNAGene.bed', sep='\\t', \n",
    "                            header=None, names=['chr', 'start', 'end', 'gene_id', 'score', 'strand'])\n",
    "noncodev6_bed = pd.read_csv('../../reference_lncRNA/human/bed/NONCODEv6_hg38.lncRNAGene.bed', sep='\\t', \n",
    "                            header=None, names=['chr', 'start', 'end', 'gene_id', 'score', 'strand'])\n",
    "\n",
    "# Initialize remaining lncRNA list\n",
    "remained_lncRNA = lncRNA[['lncRNA_id', 'gene_id', 'gene_name', 'identifier']].copy()\n",
    "results = []\n",
    "\n",
    "# Get genomic position by noncode_id (noncodev6)\n",
    "pos_lnc_noncodev6 = pd.merge(\n",
    "    remained_lncRNA,\n",
    "    noncodev6_bed[['chr', 'start', 'end', 'strand', 'gene_id']],\n",
    "    left_on='identifier',\n",
    "    right_on='gene_id',\n",
    "    how='inner'\n",
    ")\n",
    "results.append(pos_lnc_noncodev6)\n",
    "remained_lncRNA = remained_lncRNA[~remained_lncRNA['lncRNA_id'].isin(pos_lnc_noncodev6['lncRNA_id'])]\n",
    "\n",
    "# Get genomic position by noncode_id (noncodev5)\n",
    "pos_lnc_noncodev5 = pd.merge(\n",
    "    remained_lncRNA,\n",
    "    noncodev5_bed[['chr', 'start', 'end', 'strand', 'gene_id']],\n",
    "    left_on='identifier',\n",
    "    right_on='gene_id',\n",
    "    how='inner'\n",
    ")\n",
    "results.append(pos_lnc_noncodev5)\n",
    "remained_lncRNA = remained_lncRNA[~remained_lncRNA['lncRNA_id'].isin(pos_lnc_noncodev5['lncRNA_id'])]\n",
    "\n",
    "# Get genomic position by ensembl_id & gene_name\n",
    "# **STEP 1: Extract the version number from BED file**\n",
    "def extract_version(filename):\n",
    "    match = re.search(r'GRCh38\\.(\\d+)\\.bed', filename) # ensembl\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# **STEP 2: Get all BED files and sort them by version number**\n",
    "bed_files = [f for f in os.listdir(ensembl_dir) if f.endswith(\".bed\")]\n",
    "bed_files_sorted = sorted(bed_files, key=extract_version, reverse=True)  # Sort by version number in descending order\n",
    "\n",
    "# **STEP 3: Iterate over sorted BED files**\n",
    "for bed_file in bed_files_sorted:\n",
    "    bed_path = os.path.join(ensembl_dir, bed_file)\n",
    "\n",
    "    # Read ensembl BED file\n",
    "    ensembl_bed = pd.read_csv(bed_path, sep='\\t', header=None, \n",
    "                              names=['chr', 'start', 'end', 'gene_name', 'gene_id', 'strand'])\n",
    "\n",
    "    # Match by gene_id\n",
    "    pos_lnc_ensembl = pd.merge(remained_lncRNA, \n",
    "                               ensembl_bed[['chr', 'start', 'end', 'strand', 'gene_id']], \n",
    "                               left_on='identifier',\n",
    "                               right_on='gene_id', how='inner')\n",
    "    results.append(pos_lnc_ensembl)\n",
    "    remained_lncRNA = remained_lncRNA[~remained_lncRNA['lncRNA_id'].isin(pos_lnc_ensembl['lncRNA_id'])]\n",
    "\n",
    "def split_aliases(name):\n",
    "    if pd.isna(name):\n",
    "        return []\n",
    "    parts = [p.strip() for p in str(name).split(';')]\n",
    "    parts = [p for p in parts if p]\n",
    "    seen, out = set(), []\n",
    "    for p in parts:\n",
    "        if p not in seen:\n",
    "            out.append(p); seen.add(p)\n",
    "    return out\n",
    "\n",
    "for bed_file in bed_files_sorted:\n",
    "    bed_path = os.path.join(ensembl_dir, bed_file)\n",
    "\n",
    "    # Read ensembl BED file\n",
    "    ensembl_bed = pd.read_csv(bed_path, sep='\\t', header=None, \n",
    "                              names=['chr', 'start', 'end', 'gene_name', 'gene_id', 'strand'])\n",
    "\n",
    "    # Match by gene_name\n",
    "    pos_lnc_gene_name = pd.merge(remained_lncRNA, \n",
    "                                 ensembl_bed[['chr', 'start', 'end', 'strand', 'gene_name']], \n",
    "                                 left_on='identifier',\n",
    "                                 right_on='gene_name', how='inner')\n",
    "    remained_lncRNA = remained_lncRNA[~remained_lncRNA['lncRNA_id'].isin(pos_lnc_gene_name['lncRNA_id'])]\n",
    "    results.append(pos_lnc_gene_name)\n",
    "\n",
    "# Combine all results\n",
    "pos_lnc = pd.concat(results, ignore_index=True).drop_duplicates(subset=['lncRNA_id'])\n",
    "\n",
    "# Save remaining lncRNAs without genomic positions\n",
    "remained_lncRNA.drop_duplicates().to_csv('lnc_no_pos.csv', index=False)\n",
    "\n",
    "# Generate CSV files\n",
    "pos_lnc[['chr', 'start', 'end', 'strand', 'lncRNA_id']].to_csv('lncRNA_0-based.csv', index=False)\n",
    "\n",
    "## convert 0-based position to 1-based position\n",
    "pos_lnc['start'] = pos_lnc['start'] + 1\n",
    "pos_lnc[['chr', 'start', 'end', 'strand', 'lncRNA_id']].to_csv('lncRNA_1-based.csv', index=False)\n",
    "\n",
    "pos_lnc = pos_lnc[['chr', 'start', 'end', 'strand', 'lncRNA_id']]\n",
    "\n",
    "print(\"Processing complete. Results saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 Annotate histone marks, DHS and CTCF binding site for lncRNAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.1 Calculate features of epigenomic marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing heart - DHS ...\n",
      "Processing heart - H3K4me3 ...\n",
      "Processing heart - H3K9me3 ...\n",
      "Processing heart - H3K27ac ...\n",
      "Processing heart - H3K36me3 ...\n",
      "Processing heart - H3K27me3 ...\n",
      "Processing lung - DHS ...\n",
      "Processing lung - H3K4me3 ...\n",
      "Processing lung - H3K9me3 ...\n",
      "Processing lung - H3K27ac ...\n",
      "Processing lung - H3K36me3 ...\n",
      "Processing lung - H3K27me3 ...\n",
      "Processing stomach - DHS ...\n",
      "Processing stomach - H3K4me3 ...\n",
      "Processing stomach - H3K9me3 ...\n",
      "Processing stomach - H3K27ac ...\n",
      "Processing stomach - H3K36me3 ...\n",
      "Processing stomach - H3K27me3 ...\n"
     ]
    }
   ],
   "source": [
    "import pyBigWig\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "lncRNA_file = \"lncRNA_0-based.csv\"\n",
    "epi_folder = \"../../omics/ENCODE_annotation/human/\"\n",
    "\n",
    "# Read lncRNA coordinate information\n",
    "lncRNAs = pd.read_csv(lncRNA_file)\n",
    "\n",
    "# Store lncRNA IDs that failed to extract features\n",
    "failed_lncRNA_ids = set()\n",
    "\n",
    "# List of tissue and epigenetic features\n",
    "#cell_lines = ['HepG2','HeLa','K562']\n",
    "tissues = ['heart','lung','stomach']\n",
    "epi_features = ['DHS', 'H3K4me3', 'H3K9me3', 'H3K27ac', 'H3K36me3', 'H3K27me3']\n",
    "\n",
    "# Function to calculate both peak counts and max signal value\n",
    "def calculate_features(row, bb):\n",
    "    chrom = str(row[\"chr\"])\n",
    "    start = int(row[\"start\"])\n",
    "    end = int(row[\"end\"])\n",
    "    \n",
    "    try:\n",
    "        entries = bb.entries(chrom, start, end)  # Extract peak intervals\n",
    "        if entries is None:\n",
    "            return 0, 0  # No peaks, return (0, 0)\n",
    "        \n",
    "        peak_count = len(entries)  # Count peaks\n",
    "        signal_values = []\n",
    "        \n",
    "        # Extract signalValues (assuming details is space/tab-separated and signalValue is at index 6)\n",
    "        for entry in entries:\n",
    "            details = entry[2]  # Get the details string\n",
    "            details_parts = details.split()  # Split by space or tab\n",
    "            if len(details_parts) > 6:  # Ensure there are enough fields\n",
    "                signal_value = float(details_parts[6])  # NarrowPeak format: signalValue is usually at index 6\n",
    "                signal_values.append(signal_value)\n",
    "\n",
    "        max_signal_value = max(signal_values) if signal_values else 0  # Get max signalValue\n",
    "        return peak_count, max_signal_value\n",
    "    \n",
    "    except Exception as e:\n",
    "        return 0, 0  # Return (0,0) in case of failure\n",
    "\n",
    "# Iterate through all tissue folders\n",
    "for tissue in tissues:\n",
    "    # Initialize DataFrame to store all features\n",
    "    combined_features = lncRNAs.copy()\n",
    "    tissue_folder = os.path.join(epi_folder, tissue)\n",
    "    if not os.path.isdir(tissue_folder):\n",
    "        print(f\"Warning: tissue line folder {tissue_folder} does not exist, skipping.\")\n",
    "    else:\n",
    "        # Iterate through all epigenetic feature bigBed files\n",
    "        for epi_feature in epi_features:\n",
    "            bigbed_file = os.path.join(tissue_folder, f\"{epi_feature}.bigbed\")\n",
    "            if not os.path.exists(bigbed_file):\n",
    "                print(f\"Warning: File {bigbed_file} does not exist, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Open the bigBed file\n",
    "            try:\n",
    "                bb = pyBigWig.open(bigbed_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to open file {bigbed_file}, error: {e}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing {tissue} - {epi_feature} ...\")\n",
    "            \n",
    "            # Calculate both features per lncRNA region\n",
    "            results = lncRNAs.apply(calculate_features, axis=1, bb=bb)\n",
    "\n",
    "            # Store results in two separate columns\n",
    "            combined_features[f\"_{epi_feature}_peak_counts\"] = results.apply(lambda x: x[0])  # Peak counts\n",
    "            combined_features[f\"_{epi_feature}_max_signalValue\"] = results.apply(lambda x: x[1])  # Max signal value\n",
    "            \n",
    "            # Close the bigBed file\n",
    "            bb.close()\n",
    "\n",
    "    output_file = f\"{tissue}_epi.csv\"\n",
    "\n",
    "    # Remove unnecessary columns and save results\n",
    "    combined_features = combined_features.drop(columns=['chr', 'start', 'end', 'strand'])\n",
    "    combined_features.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 Get conversation score for lncRNA genes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3.1 Calculate feature of conservation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phyloP.bw ...\n",
      "Processing phastCons.bw ...\n",
      "Conservation scores calculated and saved.\n",
      "conservation_festure.csv contains 52913 records.\n",
      "no_conservation_lncRNA.csv contains 53 missing lncRNA IDs.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyBigWig\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def get_scores(df, bw_path, score_name):\n",
    "    \"\"\"\n",
    "    Extracts conservation scores from a bigWig file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing lncRNA information.\n",
    "        bw_path (str): Path to the bigWig file.\n",
    "        score_name (str): Name of the conservation score.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with mean and max scores.\n",
    "        set: Set of lncRNA_ids without scores.\n",
    "    \"\"\"\n",
    "    scores_df = df.copy()\n",
    "    scores_df[f'{score_name}_mean_score'] = np.nan\n",
    "    scores_df[f'{score_name}_max_score'] = np.nan\n",
    "\n",
    "    no_score_ids = set()\n",
    "\n",
    "    try:\n",
    "        with pyBigWig.open(bw_path) as bw:\n",
    "            for index, row in df.iterrows():\n",
    "                chrom = row['chr']\n",
    "                if chrom in bw.chroms():\n",
    "                    scores = bw.values(chrom, int(row['start']), int(row['end']), numpy=True)\n",
    "                    if scores.size > 0 and not np.all(np.isnan(scores)):\n",
    "                        scores_df.at[index, f'{score_name}_mean_score'] = np.nanmean(scores)\n",
    "                        scores_df.at[index, f'{score_name}_max_score'] = np.nanmax(scores)\n",
    "                    else:\n",
    "                        no_score_ids.add(row['lncRNA_id'])\n",
    "                else:\n",
    "                    no_score_ids.add(row['lncRNA_id'])\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to open {bw_path}: {e}\")\n",
    "\n",
    "    return scores_df, no_score_ids\n",
    "\n",
    "def calculate_conservation_features(lncRNAs_csv, conservation_dir):\n",
    "    \"\"\"\n",
    "    Calculates conservation features from all bigWig files in a given directory.\n",
    "\n",
    "    Args:\n",
    "        lncRNAs_csv (str): Path to lncRNA CSV file.\n",
    "        conservation_dir (str): Directory containing bigWig files.\n",
    "\n",
    "    Outputs:\n",
    "        - lnc_with_conservation_scores.csv: lncRNAs with conservation scores.\n",
    "        - lnc_without_conservation_scores.csv: lncRNAs without available scores.\n",
    "    \"\"\"\n",
    "    lncRNAs = pd.read_csv(lncRNAs_csv)\n",
    "    missing_scores_ids = set()\n",
    "\n",
    "    # List all bigWig files in the directory\n",
    "    bw_files = [f for f in os.listdir(conservation_dir) if f.endswith(('.bw', '.bigWig'))]\n",
    "\n",
    "    if not bw_files:\n",
    "        print(\" No bigWig files found in the directory.\")\n",
    "        return\n",
    "\n",
    "    # Initialize lnc_with_score with lncRNA IDs\n",
    "    lnc_with_score = lncRNAs.copy()\n",
    "\n",
    "    # Process each bigWig file\n",
    "    for bw_file in bw_files:\n",
    "        bw_path = os.path.join(conservation_dir, bw_file)\n",
    "        score_name = os.path.splitext(bw_file)[0]  # Use filename as score name\n",
    "\n",
    "        print(f\"Processing {bw_file} ...\")\n",
    "\n",
    "        scores_df, no_score_ids = get_scores(lnc_with_score, bw_path, score_name)\n",
    "        missing_scores_ids.update(no_score_ids)\n",
    "\n",
    "        # Merge the new scores into the main DataFrame\n",
    "        lnc_with_score = pd.merge(lnc_with_score, scores_df[[ 'lncRNA_id', \n",
    "                                                              f'{score_name}_mean_score', \n",
    "                                                              f'{score_name}_max_score']], \n",
    "                                  on='lncRNA_id', how='left')\n",
    "\n",
    "    # Remove lncRNAs without any scores\n",
    "    lnc_with_score = lnc_with_score.drop(columns=['chr', 'start', 'end', 'strand'])\n",
    "    lnc_with_score = lnc_with_score[~lnc_with_score['lncRNA_id'].isin(missing_scores_ids)]\n",
    "\n",
    "    # Save outputs\n",
    "    lnc_with_score.to_csv(\"conservation_feature.csv\", index=False)\n",
    "    pd.Series(list(missing_scores_ids)).to_csv(\"no_conservation_lncRNA.csv\", index=False, header=[\"lncRNA_id\"])\n",
    "\n",
    "    print(\"Conservation scores calculated and saved.\")\n",
    "    print(f\"conservation_festure.csv contains {len(lnc_with_score)} records.\")\n",
    "    print(f\"no_conservation_lncRNA.csv contains {len(missing_scores_ids)} missing lncRNA IDs.\")\n",
    "\n",
    "# Usage Example\n",
    "conservation_dir = '../../omics/conservation/human'  # Directory containing multiple bigWig files\n",
    "calculate_conservation_features('lncRNA_1-based.csv', conservation_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 Get sequence feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4.1 Get sequence for lncRNA genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfaidx import Fasta\n",
    "import pandas as pd\n",
    "\n",
    "# file path\n",
    "genome_fasta = \"../../reference_lncRNA/reference_genome/GRCh38.p14.genome.fa\"\n",
    "output_fasta = \"lncRNA_sequences.fasta\"\n",
    "\n",
    "chrname_mapping = pd.read_csv(\"chr_name_mapping.csv\")\n",
    "lncRNA = pd.read_csv(\"lncRNA_1-based.csv\")\n",
    "chrname_dict = dict(zip(chrname_mapping['original_name'], chrname_mapping['standard_name']))\n",
    "lncRNA['chr'] = lncRNA['chr'].replace(chrname_dict)\n",
    "\n",
    "# load reference genome\n",
    "genome = Fasta(genome_fasta)\n",
    "\n",
    "\n",
    "# open output file\n",
    "with open(output_fasta, \"w\") as fasta_out:\n",
    "    for index, row in lncRNA.iterrows():\n",
    "        chrom = row[\"chr\"]\n",
    "\n",
    "        start = int(row[\"start\"])\n",
    "        end = int(row[\"end\"])\n",
    "        strand = row[\"strand\"]\n",
    "        lncRNA_id = row[\"lncRNA_id\"]\n",
    "\n",
    "        # extract sequence\n",
    "        try:\n",
    "            seq = genome[chrom][start:end].seq\n",
    "            if strand == \"-\":\n",
    "                # extract complementary sequence if strand == \"-\"\n",
    "                seq = genome[chrom][start:end].reverse.complement.seq\n",
    "\n",
    "            fasta_out.write(f\">{lncRNA_id}\\n\")\n",
    "            fasta_out.write(seq + \"\\n\")\n",
    "\n",
    "        except KeyError:\n",
    "            print(f\"Chromosome {chrom} not found in the genome file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4.2 Calculate sequence feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to seq_feature.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "def fasta_parser(fasta_file):\n",
    "    \"\"\"\n",
    "    Custom FASTA file parser to read sequences manually.\n",
    "    \"\"\"\n",
    "    with open(fasta_file, \"r\") as file:\n",
    "        identifier = None\n",
    "        sequence_lines = []\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if identifier is not None:  # Save the previous sequence before reading a new one\n",
    "                    yield SeqRecord(Seq(''.join(sequence_lines).replace(\"\\n\", \"\")), id=identifier.strip(), description='')\n",
    "                identifier = line[1:].strip()  # Extract new sequence ID, removing '>'\n",
    "                sequence_lines = []  # Reset sequence content\n",
    "            else:\n",
    "                sequence_lines.append(line.strip())\n",
    "        if identifier is not None:  # Save the last sequence\n",
    "            yield SeqRecord(Seq(''.join(sequence_lines).replace(\"\\n\", \"\")), id=identifier.strip(), description='')\n",
    "\n",
    "# Specify the input FASTA file\n",
    "input_fasta = \"lncRNA_sequences.fasta\"  # Replace with the actual FASTA file path\n",
    "\n",
    "def calculate_cpg_and_gc(sequence, window_size=None):\n",
    "    \"\"\"\n",
    "    Calculate CpG count, CpG islands, GC content, and sequence length for a given sequence.\n",
    "    \"\"\"\n",
    "    sequence = sequence.upper()  # Ensure sequence is in uppercase\n",
    "    seq_len = len(sequence)\n",
    "    if window_size is None:\n",
    "        cpg_count = sequence.count('CG')\n",
    "        cpg_islands = len(re.findall(r'CG(CG)+', sequence))\n",
    "        gc_content = (sequence.count('G') + sequence.count('C')) / seq_len if seq_len > 0 else 0\n",
    "        return {\n",
    "            \"CpG_count\": cpg_count,\n",
    "            \"CpG_islands\": cpg_islands,\n",
    "            \"GC_content\": round(gc_content * 100, 2),\n",
    "            \"Length\": seq_len\n",
    "        }\n",
    "\n",
    "# Iterate through each sequence and compute features\n",
    "results = []\n",
    "for record in fasta_parser(input_fasta):\n",
    "    seq_id = record.id\n",
    "    sequence = str(record.seq)\n",
    "    features = calculate_cpg_and_gc(sequence)  # Compute features for the entire sequence\n",
    "    results.append({\n",
    "        \"lncRNA_id\": seq_id,\n",
    "        \"CpG_count\": features[\"CpG_count\"],\n",
    "        \"CpG_islands\": features[\"CpG_islands\"],\n",
    "        \"GC_content (%)\": features[\"GC_content\"],\n",
    "        \"Length\": features[\"Length\"]\n",
    "    })\n",
    "\n",
    "# Save computed features to a CSV file\n",
    "df = pd.DataFrame(results)\n",
    "output_csv = \"seq_feature.csv\"  # Output feature table\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"Results saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 Merge all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tissues = ['heart','lung','stomach']\n",
    "\n",
    "conservation_feature = pd.read_csv('conservation_feature.csv')\n",
    "seq_feature = pd.read_csv('seq_feature.csv')\n",
    "merged_feature = pd.merge(seq_feature, conservation_feature, on=\"lncRNA_id\", how=\"inner\")\n",
    "all_epi = merged_feature[['lncRNA_id']]\n",
    "for tissue in tissues:\n",
    "\tepi_feature = pd.read_csv(f'{tissue}_epi.csv')\n",
    "\tot_merged_feature = pd.merge(merged_feature, epi_feature, on=\"lncRNA_id\", how=\"inner\")\n",
    "\tall_epi = pd.merge(all_epi, epi_feature, on='lncRNA_id', how='inner')\n",
    "\tot_merged_feature.to_csv(f\"{tissue}_annotation.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 Filter out invalid interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/LPPI/human/lppi_uodated.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlppi_with_valid_lnc.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m inter \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minter_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m lncRNA_with_annotation \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(lncRNA_annotation_file)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Convert lncRNA_ID list to a set for faster lookup\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/LPPI/human/lppi_uodated.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "inter_file = '../../data/LPPI/human/lppi_uodated.csv'\n",
    "lncRNA_annotation_file = \"heart_annotation.csv\"\n",
    "output_file = \"lppi_with_valid_lnc.csv\"\n",
    "\n",
    "# Load data\n",
    "inter = pd.read_csv(inter_file)\n",
    "lncRNA_with_annotation = pd.read_csv(lncRNA_annotation_file)\n",
    "\n",
    "# Convert lncRNA_ID list to a set for faster lookup\n",
    "lncRNA_set = set(lncRNA_with_annotation['lncRNA_id'])\n",
    "\n",
    "# Apply filtering\n",
    "def filter_node_i(node):\n",
    "    if str(node).startswith(\"l\"):\n",
    "        return node in lncRNA_set  # Check only if it starts with 'l'\n",
    "    return True  # Keep all other nodes\n",
    "\n",
    "valid_inter = inter[inter['Node_i'].apply(filter_node_i)]\n",
    "\n",
    "# Save results\n",
    "valid_inter.to_csv(output_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esslnc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
