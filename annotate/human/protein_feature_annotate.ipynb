{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate features for proteins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 Extract all protein. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1.1 Extract all protein molecules from a filtered LPPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "inter_file = 'lppi_with_valid_lnc.csv'\n",
    "inter = pd.read_csv(inter_file)\n",
    "\n",
    "# Concatenate two columns into a new Series and remove duplicates\n",
    "molecule = pd.concat([inter['Node_i'], inter['Node_j']]).reset_index(drop=True)\n",
    "molecule_df = pd.DataFrame(molecule, columns=['molecule'])\n",
    "molecule_df = molecule_df.drop_duplicates()\n",
    "\n",
    "protein_file = '../../data/LPPI/human/protein_updated.csv'\n",
    "proteins = pd.read_csv(protein_file)\n",
    "\n",
    "proteins = proteins[proteins['protein_id'].isin(molecule_df['molecule'])]\n",
    "\n",
    "# Export to CSV file\n",
    "protein_file = 'proteins.csv'\n",
    "proteins.to_csv(protein_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1.2 Get ensembl id from BED file for protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Results saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Set the directory containing ensembl data\n",
    "ensembl_dir = \"../../reference_lncRNA/human/bed/ensembl/\"\n",
    "\n",
    "# Read protein_id and protein_name information\n",
    "proteins = pd.read_csv('./proteins.csv')\n",
    "\n",
    "# Initialize remaining protein list\n",
    "remained_protein = proteins[['protein_id', 'protein']].copy()\n",
    "results = []\n",
    "\n",
    "# Get genomic position by ensembl_id & gene_name\n",
    "# **STEP 1: Extract the version number from BED file**\n",
    "def extract_version(filename):\n",
    "    match = re.search(r'GRCh38\\.(\\d+)\\.bed', filename) # ensembl\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# **STEP 2: Get all BED files and sort them by version number**\n",
    "bed_files = [f for f in os.listdir(ensembl_dir) if f.endswith(\".bed\")]\n",
    "bed_files_sorted = sorted(bed_files, key=extract_version, reverse=True)  # Sort by version number in descending order\n",
    "\n",
    "# **STEP 3: Iterate over sorted BED files**\n",
    "for bed_file in bed_files_sorted:\n",
    "    bed_path = os.path.join(ensembl_dir, bed_file)\n",
    "\n",
    "    # Read ensembl BED file\n",
    "    ensembl_bed = pd.read_csv(bed_path, sep='\\t', header=None, \n",
    "                              names=['chr', 'start', 'end', 'gene_name', 'ensembl_id', 'strand'])\n",
    "\n",
    "    # Match by gene_id\n",
    "    ensembl_id_map = pd.merge(remained_protein, \n",
    "                               ensembl_bed[['gene_name', 'ensembl_id']], \n",
    "                               left_on='protein',\n",
    "                               right_on='gene_name', how='inner')\n",
    "    results.append(ensembl_id_map)\n",
    "    remained_protein = remained_protein[~remained_protein['protein_id'].isin(ensembl_id_map['protein_id'])]\n",
    "\n",
    "# Combine all results\n",
    "pro_ens = pd.concat(results, ignore_index=True).drop_duplicates(subset=['protein_id'])\n",
    "\n",
    "# Save remaining proteins without genomic positions\n",
    "remained_protein.drop_duplicates().to_csv('pro_no_id.csv', index=False)\n",
    "\n",
    "# Generate CSV files\n",
    "pro_ens[['protein', 'protein_id', 'ensembl_id']].to_csv('pro_ens_map.csv', index=False)\n",
    "print(\"Processing complete. Results saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 Annotate features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.1 Annotate the number of go terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def count_gene_go_terms(go_file):\n",
    "\n",
    "    df = pd.read_csv(go_file, dtype=str)\n",
    "    \n",
    "    # Keep only gene symbols and GO terms, remove duplicates\n",
    "    df_filtered = df[[\"Gene name\", \"GO term accession\"]].drop_duplicates()\n",
    "    \n",
    "    # Count the number of unique GO terms associated with each gene\n",
    "    gene_go_counts = df_filtered.groupby([\"Gene name\"])[\"GO term accession\"].nunique().reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    gene_go_counts.columns = [\"gene_name\", \"GO_Term_Count\"]\n",
    "    \n",
    "    return gene_go_counts\n",
    "\n",
    "go_file = \"../../features/protein/human/go.csv\"  \n",
    "gene_go_counts = count_gene_go_terms(go_file)\n",
    "\n",
    "protein = pd.read_csv(\"proteins.csv\", dtype=str)\n",
    "\n",
    "filtered_gene_go_term_counts = pd.merge(protein, gene_go_counts, left_on='protein', right_on='gene_name', how='inner')\n",
    "filtered_gene_go_term_counts = filtered_gene_go_term_counts[['protein_id','GO_Term_Count']]\n",
    "\n",
    "# Save the result to a CSV file\n",
    "filtered_gene_go_term_counts.to_csv(\"protein_go_term_counts.csv\", sep=\",\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.2 Annotate the number of orthologs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2.2.1 Get ortholog count using Ensembl REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Configuration parameters\n",
    "INPUT_FILE = \"proteins.csv\"\n",
    "OUTPUT_FILE = \"all_species_ortholog_counts.csv\"\n",
    "THREADS = 5\n",
    "MAX_RETRIES = 3\n",
    "SLEEP_BETWEEN = 0.4\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "def count_orthologs_by_symbol(symbol):\n",
    "    url = f\"https://rest.ensembl.org/homology/symbol/homo_sapiens/{symbol}?type=orthologues\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                homologies = data['data'][0].get('homologies', [])\n",
    "                count = sum(1 for h in homologies if h['type'] == 'ortholog_one2one')\n",
    "                return (symbol, count) \n",
    "            elif r.status_code == 404:\n",
    "                return None          \n",
    "            else:\n",
    "                print(f\"[{symbol}] HTTP {r.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{symbol}] error: {e}\")\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "    return None                 \n",
    "\n",
    "def load_symbols():\n",
    "    with open(INPUT_FILE, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return [row['protein'] for row in reader if row.get('protein')]\n",
    "\n",
    "def load_existing_results():\n",
    "    if not os.path.exists(OUTPUT_FILE):\n",
    "        return set()\n",
    "    df = pd.read_csv(OUTPUT_FILE)\n",
    "    return set(df['gene_name'].values)\n",
    "\n",
    "def save_result(symbol, count):\n",
    "    with open(OUTPUT_FILE, \"a\") as f:\n",
    "        f.write(f\"{symbol},{count}\\n\")\n",
    "\n",
    "def main():\n",
    "    all_symbols = load_symbols()\n",
    "    done_symbols = load_existing_results()\n",
    "    symbols_to_query = [s for s in all_symbols if s not in done_symbols]\n",
    "\n",
    "    print(f\"\\n Total genes: {len(all_symbols)}\")\n",
    "    print(f\" Already processed: {len(done_symbols)}\")\n",
    "    print(f\" Pending: {len(symbols_to_query)}\")\n",
    "    print(f\" Starting concurrent queries with {THREADS} threads\\n\")\n",
    "\n",
    "    if not os.path.exists(OUTPUT_FILE) or os.stat(OUTPUT_FILE).st_size == 0:\n",
    "        with open(OUTPUT_FILE, \"w\") as f:\n",
    "            f.write(\"gene_name,ortholog_count\\n\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
    "        future_to_symbol = {\n",
    "            executor.submit(count_orthologs_by_symbol, symbol): symbol\n",
    "            for symbol in symbols_to_query\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_symbol):\n",
    "            symbol = future_to_symbol[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result: \n",
    "                    save_result(*result)\n",
    "                    print(f\" {result[0]} â†’ {result[1]} orthologs\")\n",
    "            except Exception as exc:\n",
    "                print(f\" {symbol} exception: {exc}\")\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    print(f\"\\n Query complete! Results saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2.2.2 Annotate ortholog count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "orthologs_counts = pd.read_csv('all_species_ortholog_counts.csv')\n",
    "protein = pd.read_csv('proteins.csv')\n",
    "\n",
    "orthologs_counts = orthologs_counts[['gene_name','ortholog_count']]\n",
    "protein_orthologs_counts = pd.merge(protein,orthologs_counts,left_on='protein',right_on='gene_name',how='inner')\n",
    "protein_orthologs_counts = protein_orthologs_counts[['protein_id','ortholog_count']]\n",
    "protein_orthologs_counts.to_csv(\"protein_orthologs_counts.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.3 Annotate the expression feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[heart] saved protein_exp_heart.csv, n=18264\n",
      "[lung] saved protein_exp_lung.csv, n=18264\n",
      "[stomach] saved protein_exp_stomach.csv, n=18264\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "protein = pd.read_csv(\"pro_ens_map.csv\", dtype=str)\n",
    "\n",
    "for t in ['heart', 'lung', 'stomach']:\n",
    "    exp_file = pd.read_csv(\n",
    "        f\"../../features/protein/human/exp_{t}.tsv\",\n",
    "        sep=\"\\t\",\n",
    "        dtype=str\n",
    "    )\n",
    "\n",
    "    exp_file = exp_file[['gene_id', 'TPM', 'pme_TPM']].copy()\n",
    "\n",
    "    # 1) keep ENSG only\n",
    "    exp_file = exp_file[exp_file['gene_id'].str.startswith('ENSG', na=False)].copy()\n",
    "\n",
    "    # 2) mark PAR_Y using original gene_id\n",
    "    exp_file[\"is_par_y\"] = exp_file[\"gene_id\"].str.endswith(\"_PAR_Y\")\n",
    "\n",
    "    # 3) define base gene id by removing _PAR_Y only\n",
    "    exp_file[\"gene_id_base\"] = exp_file[\"gene_id\"].str.replace(r\"_PAR_Y$\", \"\", regex=True)\n",
    "\n",
    "    # 4) canonical-first selection:\n",
    "    #    ENSG... > ENSG..._PAR_Y\n",
    "    exp_file = exp_file.sort_values(\n",
    "        [\"gene_id_base\", \"is_par_y\"],\n",
    "        ascending=[True, True]\n",
    "    )\n",
    "    exp_file = exp_file.drop_duplicates(\n",
    "        subset=[\"gene_id_base\"],\n",
    "        keep=\"first\"\n",
    "    )\n",
    "\n",
    "    # 5) remove version suffix (.xx)\n",
    "    exp_file[\"gene_id_base\"] = (\n",
    "        exp_file[\"gene_id_base\"]\n",
    "        .str.replace(r\"\\.\\d+$\", \"\", regex=True)\n",
    "    )\n",
    "\n",
    "    # 6) merge with protein mapping\n",
    "    protein_exp = pd.merge(\n",
    "        protein,\n",
    "        exp_file[[\"gene_id_base\", \"pme_TPM\"]],\n",
    "        left_on=\"ensembl_id\",\n",
    "        right_on=\"gene_id_base\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    protein_exp = protein_exp[['protein_id', 'pme_TPM']].drop_duplicates()\n",
    "    protein_exp.to_csv(f\"protein_exp_{t}.csv\", index=False)\n",
    "\n",
    "    print(f\"[{t}] saved protein_exp_{t}.csv, n={protein_exp.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 Merge annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "go_anno = pd.read_csv(\"protein_go_term_counts.csv\")\n",
    "homo_anno = pd.read_csv(\"protein_orthologs_counts.csv\")\n",
    "for t in ['heart','lung','stomach']:\n",
    "\texp_anno = pd.read_csv(f\"protein_exp_{t}.csv\")\n",
    "\tprotein_annotation = pd.merge(exp_anno, go_anno, on='protein_id', how=\"inner\")\n",
    "\tprotein_annotation = protein_annotation.merge(homo_anno, on=\"protein_id\", how='inner')\n",
    "\n",
    "\tID_column = protein_annotation.iloc[:, [0]]\n",
    "\tfeature_columns = protein_annotation.iloc[:, 1:]\n",
    "\n",
    "\tscaler = StandardScaler()\n",
    "\tnormalized_data = scaler.fit_transform(feature_columns)\n",
    "\n",
    "\tdf_normalized = pd.concat([ID_column, pd.DataFrame(normalized_data, columns=feature_columns.columns)], axis=1)\n",
    "\n",
    "\tdf_normalized.to_csv(f\"protein_annotation_{t}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 Get inter with valid protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "inter = pd.read_csv(\"lppi_with_valid_lnc.csv\")\n",
    "valid_protein = pd.read_csv(f\"protein_annotation_heart.csv\")\n",
    "\n",
    "valid_protein_set = set(valid_protein['protein_id'])\n",
    "\n",
    "# Define a function to check whether nodes starting with 'p' exist in valid_protein\n",
    "def check_valid_protein(node):\n",
    "    if node.startswith('p'):\n",
    "        return node in valid_protein_set\n",
    "    return True  # Keep nodes that do not start with 'p'\n",
    "\n",
    "# Check each row: both Node_i and Node_j must be valid\n",
    "inter = inter[inter['Node_i'].apply(check_valid_protein) & inter['Node_j'].apply(check_valid_protein)]\n",
    "\n",
    "tissues = ['heart', 'lung', 'stomach']\n",
    "for tissue in tissues:\n",
    "    lnc = pd.read_csv(f\"{tissue}_annotation.csv\")\n",
    "    lnc = lnc[lnc['lncRNA_id'].isin(inter['Node_i'])]\n",
    "\n",
    "    lnc.to_csv(f\"valid_{tissue}_annotation.csv\", index=False)\n",
    "\n",
    "inter.columns = ['source', 'target']\n",
    "#inter.to_csv(\"valid_inter.csv\", index=False)\n",
    "\n",
    "inter['weight'] = 1\n",
    "inter.to_csv(\"unweighted_inter.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ELE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
