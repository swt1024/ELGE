{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate features for proteins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 Annotate pLI and pLoF score for valid proteins. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1.1 Extract all protein molecules from a filtered LPPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "inter_file = 'lppi_with_valid_lnc.csv'\n",
    "inter = pd.read_csv(inter_file)\n",
    "\n",
    "# Concatenate two columns into a new Series and remove duplicates\n",
    "molecule = pd.concat([inter['Node_i'], inter['Node_j']]).reset_index(drop=True)\n",
    "molecule_df = pd.DataFrame(molecule, columns=['molecule'])\n",
    "molecule_df = molecule_df.drop_duplicates()\n",
    "\n",
    "protein_file = '../../data/LPPI/human/protein.csv'\n",
    "proteins = pd.read_csv(protein_file)\n",
    "\n",
    "proteins = proteins[proteins['protein_id'].isin(molecule_df['molecule'])]\n",
    "\n",
    "# Export to CSV file\n",
    "protein_file = 'proteins.csv'\n",
    "proteins.to_csv(protein_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1.2 Annotate pLI score for valid proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "valid_protein = pd.read_csv(\"proteins.csv\", dtype=str)\n",
    "pLoF = pd.read_csv(\"../../omics/protein/human/pLoF_v2.txt\", sep='\\t')\n",
    "\n",
    "# Filtering required columns from pLoF\n",
    "pLoF = pLoF[['gene', 'obs_lof', 'exp_lof', 'oe_lof', 'lof_z', 'pLI']]\n",
    "#pLoF = pLoF[['gene', 'lof.obs', 'lof.exp', 'lof.oe', 'lof.z_score', 'lof.pLI']]\n",
    "\n",
    "# Merging on 'protein' column from valid_protein and 'gene' column from pLoF\n",
    "protein_pLI = pd.merge(valid_protein, pLoF, left_on='protein', right_on='gene', how='inner')\n",
    "protein_pLI = protein_pLI.drop(columns=['gene', 'protein'])\n",
    "\n",
    "# Extract rows containing NaN values in protein_pLI\n",
    "na_score_protein = protein_pLI[protein_pLI.isna().any(axis=1)]\n",
    "na_score_protein = na_score_protein[['protein_id']]\n",
    "\n",
    "protein_pLI_cleaned = protein_pLI.dropna()\n",
    "protein_invalid_score = valid_protein[~valid_protein['protein_id'].isin(protein_pLI_cleaned['protein_id'])]\n",
    "\n",
    "pLI_means = protein_pLI_cleaned.groupby('protein_id').mean().reset_index()\n",
    "\n",
    "# Save results\n",
    "pLI_means.to_csv('protein_annotation.csv', index=False)\n",
    "na_score_protein.to_csv('NA_score_protein.csv', index=False)  # Saving rows with NaN values\n",
    "protein_invalid_score.to_csv('invalid_score_protein.csv', index=False)  # Saving rows with NaN values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1.3 Convert oe_lof&lof_z to p-value and get log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import poisson, norm\n",
    "\n",
    "# Load the annotation\n",
    "df = pd.read_csv(\"protein_annotation.csv\")\n",
    "\n",
    "# Calculate p-value for oe_lof using the Poisson distribution\n",
    "df[\"oe_lof_pval\"] = df.apply(lambda row: poisson.cdf(row['obs_lof'], row['exp_lof']), axis=1)\n",
    "\n",
    "# Calculate p-value for lof_z using a two-tailed Z-test\n",
    "df[\"lof_z_pval\"] = df[\"lof_z\"].apply(lambda x: 2 * (1 - norm.cdf(abs(x))) if pd.notna(x) else np.nan)\n",
    "\n",
    "# Apply a log10 transformation to p-values with a small number adjustment\n",
    "df[\"log_oe_lof_pval\"] = df[\"oe_lof_pval\"].apply(lambda x: np.log10(x + 1e-10))\n",
    "df[\"log_lof_z_pval\"] = df[\"lof_z_pval\"].apply(lambda x: np.log10(x + 1e-10))\n",
    "df[\"log_pLI\"] = df[\"pLI\"].apply(lambda x: np.log10(x) if pd.notna(x) and x > 0 else np.nan)\n",
    "\n",
    "# Select columns to keep\n",
    "df = df[['protein_id', \"log_oe_lof_pval\", \"log_lof_z_pval\", \"log_pLI\"]]\n",
    "\n",
    "# Save the transformed annotation\n",
    "df.to_csv(\"transformed_protein_annotation.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1.4 Delete interaction with protein which have NA pLI score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'inter_with_valid_lnc.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m inter \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minter_with_valid_lnc.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m invalid_protein \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid_score_protein.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m NA_protein \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNA_score_protein.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'inter_with_valid_lnc.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "inter = pd.read_csv(\"inter_with_valid_lnc.csv\")\n",
    "invalid_protein = pd.read_csv(\"invalid_score_protein.csv\")\n",
    "NA_protein = pd.read_csv(\"NA_score_protein.csv\")\n",
    "\n",
    "inter = inter[~inter['Node_i'].isin(invalid_protein['protein_id'])]\n",
    "inter = inter[~inter['Node_j'].isin(invalid_protein['protein_id'])]\n",
    "\n",
    "inter = inter[~inter['Node_i'].isin(NA_protein['protein_id'])]\n",
    "inter = inter[~inter['Node_j'].isin(NA_protein['protein_id'])]\n",
    "\n",
    "inter.columns = ['source', 'target', 'weight']\n",
    "inter.to_csv(\"valid_inter.csv\", index=False)\n",
    "\n",
    "tissues = ['heart','lung','stomach']\n",
    "for tissue in tissues:\n",
    "\tlnc = pd.read_csv(f\"{tissue}_annotation.csv\")\n",
    "\tlnc = lnc[lnc['lncRNA_id'].isin(inter['Node_i'])]\n",
    "\n",
    "\tlnc.to_csv(f\"valid_{tissue}_annotation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GO数量+直系同源基因数量+表达量做特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据蛋白质名称获取ensembl id(from BED file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Results saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Set the directory containing ensembl data\n",
    "ensembl_dir = \"../../reference_lncRNA/human/bed/ensembl/\"\n",
    "\n",
    "# Read protein_id and protein_name information\n",
    "proteins = pd.read_csv('./proteins.csv')\n",
    "\n",
    "# Initialize remaining protein list\n",
    "remained_protein = proteins[['protein_id', 'protein']].copy()\n",
    "results = []\n",
    "\n",
    "# Get genomic position by ensembl_id & gene_name\n",
    "# **STEP 1: Extract the version number from BED file**\n",
    "def extract_version(filename):\n",
    "    match = re.search(r'GRCh38\\.(\\d+)\\.bed', filename) # ensembl\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# **STEP 2: Get all BED files and sort them by version number**\n",
    "bed_files = [f for f in os.listdir(ensembl_dir) if f.endswith(\".bed\")]\n",
    "bed_files_sorted = sorted(bed_files, key=extract_version, reverse=True)  # Sort by version number in descending order\n",
    "\n",
    "# **STEP 3: Iterate over sorted BED files**\n",
    "for bed_file in bed_files_sorted:\n",
    "    bed_path = os.path.join(ensembl_dir, bed_file)\n",
    "\n",
    "    # Read ensembl BED file\n",
    "    ensembl_bed = pd.read_csv(bed_path, sep='\\t', header=None, \n",
    "                              names=['chr', 'start', 'end', 'gene_name', 'ensembl_id', 'strand'])\n",
    "\n",
    "    # Match by gene_id\n",
    "    ensembl_id_map = pd.merge(remained_protein, \n",
    "                               ensembl_bed[['gene_name', 'ensembl_id']], \n",
    "                               left_on='protein',\n",
    "                               right_on='gene_name', how='inner')\n",
    "    results.append(ensembl_id_map)\n",
    "    remained_protein = remained_protein[~remained_protein['protein_id'].isin(ensembl_id_map['protein_id'])]\n",
    "\n",
    "# Combine all results\n",
    "pro_ens = pd.concat(results, ignore_index=True).drop_duplicates(subset=['protein_id'])\n",
    "\n",
    "# Save remaining proteins without genomic positions\n",
    "remained_protein.drop_duplicates().to_csv('pro_no_id.csv', index=False)\n",
    "\n",
    "# Generate CSV files\n",
    "pro_ens[['protein', 'protein_id', 'ensembl_id']].to_csv('pro_ens_msp.csv', index=False)\n",
    "print(\"Processing complete. Results saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取GO数量 √"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPSConnectionPool(host='www.ensembl.org', port=443): Max retries exceeded with url: /biomart/martservice (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fac311ebb10>, 'Connection to www.ensembl.org timed out. (connect timeout=60)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/urllib3/connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/urllib3/connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/urllib3/connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/urllib3/connection.py:704\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    703\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 704\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/urllib3/connection.py:207\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    210\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x7fac311ebb10>, 'Connection to www.ensembl.org timed out. (connect timeout=60)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.ensembl.org', port=443): Max retries exceeded with url: /biomart/martservice (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fac311ebb10>, 'Connection to www.ensembl.org timed out. (connect timeout=60)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 148\u001b[0m\n\u001b[1;32m    146\u001b[0m batch \u001b[38;5;241m=\u001b[39m id_list[i:i \u001b[38;5;241m+\u001b[39m BATCH_SIZE]\n\u001b[1;32m    147\u001b[0m xml \u001b[38;5;241m=\u001b[39m build_query_xml(dataset, batch)\n\u001b[0;32m--> 148\u001b[0m tsv \u001b[38;5;241m=\u001b[39m \u001b[43mpost_biomart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tsv \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tsv\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 89\u001b[0m, in \u001b[0;36mpost_biomart\u001b[0;34m(xml_query)\u001b[0m\n\u001b[1;32m     87\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(BACKOFF \u001b[38;5;241m*\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m last_exc:\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m last_exc\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBioMart request failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 82\u001b[0m, in \u001b[0;36mpost_biomart\u001b[0;34m(xml_query)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(RETRY):\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[43mSESSION\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBIOMART_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTIMEOUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m r\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/requests/sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/esslnc/lib/python3.11/site-packages/requests/adapters.py:688\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[0;32m--> 688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectTimeout\u001b[0m: HTTPSConnectionPool(host='www.ensembl.org', port=443): Max retries exceeded with url: /biomart/martservice (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fac311ebb10>, 'Connection to www.ensembl.org timed out. (connect timeout=60)'))"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fetch GO annotations from Ensembl BioMart by Ensembl Gene IDs (from input column `ensembl_id`)\n",
    "and produce TOTAL unique GO counts per protein_id.\n",
    "\n",
    "Input CSV  (must contain columns): protein_id, protein, ensembl_id\n",
    "Output CSV (columns): protein_id, go_count\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# ===================== User settings =====================\n",
    "INPUT_CSV  = \"pro_ens_msp.csv\"          # must have columns: protein_id, protein, ensembl_id\n",
    "OUTPUT_CSV = \"pro_go_counts.csv\"\n",
    "\n",
    "BIOMART_URL = \"https://www.ensembl.org/biomart/martservice\"\n",
    "\n",
    "# Datasets per species (extend as needed)\n",
    "DATASET_FOR_PREFIX: Dict[str, str] = {\n",
    "    \"ENSG\":   \"hsapiens_gene_ensembl\",   # human\n",
    "    \"ENSMUSG\":\"mmusculus_gene_ensembl\",  # mouse\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "TIMEOUT    = 60\n",
    "RETRY      = 3\n",
    "BACKOFF    = 0.6\n",
    "# ========================================================\n",
    "\n",
    "SESSION = requests.Session()  # reuse HTTP connections\n",
    "\n",
    "\n",
    "def clean_ensembl_id(x: str) -> str:\n",
    "    \"\"\"Strip spaces and remove version suffix (e.g., ENSG... .16 -> ENSG...).\"\"\"\n",
    "    if not isinstance(x, str):\n",
    "        return \"\"\n",
    "    x = x.strip()\n",
    "    if not x:\n",
    "        return \"\"\n",
    "    return re.sub(r\"\\.\\d+$\", \"\", x)\n",
    "\n",
    "\n",
    "def detect_dataset(ensembl_id: str) -> str | None:\n",
    "    \"\"\"Choose BioMart dataset by ID prefix (ENSG/ENSMUSG).\"\"\"\n",
    "    for pref, ds in DATASET_FOR_PREFIX.items():\n",
    "        if ensembl_id.startswith(pref):\n",
    "            return ds\n",
    "    return None\n",
    "\n",
    "\n",
    "def build_query_xml(dataset: str, ensg_list: List[str]) -> str:\n",
    "    \"\"\"Build BioMart XML for TSV output: ensembl_gene_id, external_gene_name, go_id.\"\"\"\n",
    "    values_xml = \"\".join(f\"<Value>{gid}</Value>\" for gid in ensg_list if gid)\n",
    "    return f\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<!DOCTYPE Query>\n",
    "<Query virtualSchemaName=\"default\" formatter=\"TSV\" header=\"0\" uniqueRows=\"1\" datasetConfigVersion=\"0.6\">\n",
    "  <Dataset name=\"{dataset}\" interface=\"default\">\n",
    "    <Filter name=\"ensembl_gene_id\" excluded=\"0\">\n",
    "      {values_xml}\n",
    "    </Filter>\n",
    "    <Attribute name=\"ensembl_gene_id\"/>\n",
    "    <Attribute name=\"external_gene_name\"/>\n",
    "    <Attribute name=\"go_id\"/>\n",
    "  </Dataset>\n",
    "</Query>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def post_biomart(xml_query: str) -> str:\n",
    "    \"\"\"POST XML to BioMart and return TSV text with simple retry/backoff.\"\"\"\n",
    "    params = {\"query\": xml_query}\n",
    "    last_exc = None\n",
    "    for i in range(RETRY):\n",
    "        try:\n",
    "            r = SESSION.post(BIOMART_URL, data=params, timeout=TIMEOUT)\n",
    "            if r.status_code == 200 and r.text is not None:\n",
    "                return r.text\n",
    "        except requests.RequestException as e:\n",
    "            last_exc = e\n",
    "        time.sleep(BACKOFF * (i + 1))\n",
    "    if last_exc:\n",
    "        raise last_exc\n",
    "    raise RuntimeError(\"BioMart request failed\")\n",
    "\n",
    "\n",
    "# ---------- Load input ----------\n",
    "df_in = pd.read_csv(INPUT_CSV, dtype=str)\n",
    "required_cols = {\"protein_id\", \"protein\", \"ensembl_id\"}\n",
    "missing = required_cols - set(df_in.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Input CSV missing required columns: {sorted(missing)}\")\n",
    "\n",
    "# Normalize Ensembl IDs (strip + remove version)\n",
    "df_in[\"ensembl_id_clean\"] = df_in[\"ensembl_id\"].fillna(\"\").astype(str).map(clean_ensembl_id)\n",
    "\n",
    "# Build protein_id -> ensembl_id mapping\n",
    "# If a protein_id appears multiple times with different non-empty Ensembl IDs,\n",
    "# we keep the first non-empty and warn.\n",
    "grp = df_in.groupby(\"protein_id\", dropna=False)[\"ensembl_id_clean\"].apply(\n",
    "    lambda s: [x for x in s if x]\n",
    ").rename(\"ensembl_ids_nonempty\")\n",
    "\n",
    "protein_to_ensembl: Dict[str, str] = {}\n",
    "ambiguous = []\n",
    "for pid, id_list in grp.items():\n",
    "    if not id_list:\n",
    "        protein_to_ensembl[pid] = \"\"  # no ID available\n",
    "    else:\n",
    "        uniq = sorted(set(id_list))\n",
    "        protein_to_ensembl[pid] = uniq[0]\n",
    "        if len(uniq) > 1:\n",
    "            ambiguous.append((pid, uniq))\n",
    "\n",
    "if ambiguous:\n",
    "    print(f\"[WARN] {len(ambiguous)} protein_id have multiple distinct Ensembl IDs. \"\n",
    "          f\"Using the first after sorting. Example: {ambiguous[0]}\")\n",
    "\n",
    "# Unique Ensembl IDs to query\n",
    "unique_ids = sorted({eid for eid in protein_to_ensembl.values() if eid})\n",
    "\n",
    "# Group IDs by dataset\n",
    "groups: Dict[str, List[str]] = {}\n",
    "unrecognized = []\n",
    "for eid in unique_ids:\n",
    "    ds = detect_dataset(eid)\n",
    "    if ds:\n",
    "        groups.setdefault(ds, []).append(eid)\n",
    "    else:\n",
    "        unrecognized.append(eid)\n",
    "\n",
    "if unrecognized:\n",
    "    print(f\"[WARN] Unrecognized Ensembl ID prefixes (not queried): {len(unrecognized)} \"\n",
    "          f\"examples: {unrecognized[:5]}  (extend DATASET_FOR_PREFIX if needed)\")\n",
    "\n",
    "# ---------- Query BioMart ----------\n",
    "parts = []\n",
    "for dataset, id_list in groups.items():\n",
    "    for i in range(0, len(id_list), BATCH_SIZE):\n",
    "        batch = id_list[i:i + BATCH_SIZE]\n",
    "        xml = build_query_xml(dataset, batch)\n",
    "        tsv = post_biomart(xml)\n",
    "        if not tsv or not tsv.strip():\n",
    "            continue\n",
    "        part = pd.read_csv(\n",
    "            io.StringIO(tsv),\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\"ensembl_id\", \"external_gene_name\", \"go_id\"],\n",
    "            dtype=str,\n",
    "        )\n",
    "        parts.append(part)\n",
    "\n",
    "ann = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame(\n",
    "    columns=[\"ensembl_id\", \"external_gene_name\", \"go_id\"], dtype=str\n",
    ")\n",
    "ann = ann.fillna(\"\")\n",
    "ann = ann[(ann[\"ensembl_id\"] != \"\") & (ann[\"go_id\"] != \"\")]\n",
    "\n",
    "# Unique GO per Ensembl ID\n",
    "ann_unique = ann.drop_duplicates(subset=[\"ensembl_id\", \"go_id\"])\n",
    "go_counts_by_eid = (\n",
    "    ann_unique.groupby(\"ensembl_id\")[\"go_id\"].nunique().rename(\"go_count\")\n",
    ")\n",
    "\n",
    "# ---------- Map back to protein_id ----------\n",
    "out = pd.DataFrame({\"protein_id\": list(protein_to_ensembl.keys())})\n",
    "out[\"ensembl_id\"] = out[\"protein_id\"].map(protein_to_ensembl)\n",
    "\n",
    "out = out.merge(go_counts_by_eid, left_on=\"ensembl_id\", right_index=True, how=\"left\")\n",
    "out[\"go_count\"] = out[\"go_count\"].fillna(0).astype(int)\n",
    "\n",
    "# Only keep required columns\n",
    "out = out[[\"protein_id\", \"go_count\"]]\n",
    "\n",
    "# Save\n",
    "Path(OUTPUT_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
    "out.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"[DONE] Wrote {len(out)} rows to {OUTPUT_CSV}\")\n",
    "print(f\"[INFO] Datasets used: {', '.join(groups.keys()) or 'None'} | \"\n",
    "      f\"Unrecognized Ensembl IDs: {len(unrecognized)} | \"\n",
    "      f\"Proteins without Ensembl ID: {sum(1 for v in protein_to_ensembl.values() if not v)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取直系同源等位基因的数量   √"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Configuration parameters\n",
    "INPUT_FILE = \"protein_name.txt\"\n",
    "OUTPUT_FILE = \"all_species_ortholog_counts.csv\"\n",
    "THREADS = 5\n",
    "MAX_RETRIES = 3\n",
    "SLEEP_BETWEEN = 0.4\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "def count_orthologs_by_symbol(symbol):\n",
    "    url = f\"https://rest.ensembl.org/homology/symbol/homo_sapiens/{symbol}?type=orthologues\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                homologies = data['data'][0].get('homologies', [])\n",
    "                count = sum(1 for h in homologies if h['type'] == 'ortholog_one2one')\n",
    "                return (symbol, count, \"OK\")\n",
    "            elif r.status_code == 404:\n",
    "                return (symbol, 0, \"NotFound\")\n",
    "            else:\n",
    "                print(f\"[{symbol}] HTTP {r.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{symbol}] error: {e}\")\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "    return (symbol, 0, \"Error\")\n",
    "\n",
    "def load_symbols():\n",
    "    with open(INPUT_FILE) as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def load_existing_results():\n",
    "    if not os.path.exists(OUTPUT_FILE):\n",
    "        return set()\n",
    "    df = pd.read_csv(OUTPUT_FILE)\n",
    "    return set(df['GeneSymbol'].values)\n",
    "\n",
    "def save_result(symbol, count, status):\n",
    "    with open(OUTPUT_FILE, \"a\") as f:\n",
    "        f.write(f\"{symbol},{count},{status}\\n\")\n",
    "\n",
    "def main():\n",
    "    all_symbols = load_symbols()\n",
    "    done_symbols = load_existing_results()\n",
    "    symbols_to_query = [s for s in all_symbols if s not in done_symbols]\n",
    "\n",
    "    print(f\"\\n Total genes: {len(all_symbols)}\")\n",
    "    print(f\" Already processed: {len(done_symbols)}\")\n",
    "    print(f\" Pending: {len(symbols_to_query)}\")\n",
    "    print(f\" Starting concurrent queries with {THREADS} threads\\n\")\n",
    "\n",
    "    with open(OUTPUT_FILE, \"a\") as f:\n",
    "        if os.stat(OUTPUT_FILE).st_size == 0:\n",
    "            f.write(\"GeneSymbol,OrthologCount,Status\\n\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
    "        future_to_symbol = {\n",
    "            executor.submit(count_orthologs_by_symbol, symbol): symbol\n",
    "            for symbol in symbols_to_query\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_symbol):\n",
    "            symbol = future_to_symbol[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                save_result(*result)\n",
    "                print(f\" {result[0]} → {result[1]} orthologs\")\n",
    "            except Exception as exc:\n",
    "                print(f\" {symbol} exception: {exc}\")\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    print(f\"\\n Query complete! Results saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取表达量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as: fetch_expression_gtex.py\n",
    "# usage:\n",
    "#   python fetch_expression_gtex.py --map mapping.csv --out-prefix expr\n",
    "#\n",
    "# Outputs:\n",
    "#   expr.summary.csv  -> one row per input protein, with overall median & mean of tissue medians\n",
    "#   expr.long.csv     -> per-tissue median TPM (long format) for each gene\n",
    "#\n",
    "# Input mapping.csv columns:\n",
    "#   input_protein, symbol, uniprot_acc, ensembl_gene\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple, List, Dict, Any\n",
    "\n",
    "GTEX_EXPR = \"https://gtexportal.org/api/v2/gene/medianExpression\"\n",
    "HEADERS_JSON = {\"Accept\": \"application/json\"}\n",
    "\n",
    "def request_with_retry(url: str, params: dict, headers: dict = None, max_try: int = 3, timeout: int = 60):\n",
    "    \"\"\"Simple retry wrapper for HTTP GET.\"\"\"\n",
    "    for i in range(max_try):\n",
    "        r = requests.get(url, params=params, headers=headers, timeout=timeout)\n",
    "        if r.status_code == 200:\n",
    "            return r\n",
    "        time.sleep(0.7 * (i + 1))\n",
    "    return r\n",
    "\n",
    "def strip_version(ensg: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Drop version suffix from Ensembl gene ID if present (e.g., ENSG... .12).\"\"\"\n",
    "    if not ensg or not isinstance(ensg, str):\n",
    "        return ensg\n",
    "    return ensg.split(\".\")[0]\n",
    "\n",
    "def gtex_expression_summary(ensembl_gene: str) -> Tuple[Optional[float], Optional[float], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Fetch GTEx v8 per-tissue median TPM for a gene, then compute:\n",
    "      - overall_median_of_tissue_medians\n",
    "      - overall_mean_of_tissue_medians\n",
    "    Returns:\n",
    "      (overall_median, overall_mean, details_list)\n",
    "    details_list: [{\"tissue\": str, \"median_tpm\": float, \"unit\": str}, ...]\n",
    "    \"\"\"\n",
    "    if not ensembl_gene:\n",
    "        return None, None, []\n",
    "    # Try with full ID first; fallback to non-version ID if necessary.\n",
    "    ensg_full = ensembl_gene\n",
    "    params = {\"gencodeId\": ensg_full, \"format\": \"json\"}\n",
    "    r = request_with_retry(GTEX_EXPR, params=params, headers=HEADERS_JSON)\n",
    "    rows = []\n",
    "    if r.status_code == 200:\n",
    "        rows = (r.json() or {}).get(\"data\") or []\n",
    "    if not rows:\n",
    "        ensg_nover = strip_version(ensembl_gene)\n",
    "        if ensg_nover and ensg_nover != ensg_full:\n",
    "            params = {\"gencodeId\": ensg_nover, \"format\": \"json\"}\n",
    "            r2 = request_with_retry(GTEX_EXPR, params=params, headers=HEADERS_JSON)\n",
    "            if r2.status_code == 200:\n",
    "                rows = (r2.json() or {}).get(\"data\") or []\n",
    "\n",
    "    if not rows:\n",
    "        return None, None, []\n",
    "\n",
    "    details = []\n",
    "    for row in rows:\n",
    "        details.append({\n",
    "            \"tissue\": row.get(\"tissueSiteDetail\"),\n",
    "            \"median_tpm\": row.get(\"median\"),\n",
    "            \"unit\": row.get(\"unit\")\n",
    "        })\n",
    "\n",
    "    medians = [d[\"median_tpm\"] for d in details if isinstance(d.get(\"median_tpm\"), (int, float))]\n",
    "    if not medians:\n",
    "        return None, None, details\n",
    "\n",
    "    s = pd.Series(medians)\n",
    "    overall_median = float(s.median())\n",
    "    overall_mean = float(s.mean())\n",
    "    return overall_median, overall_mean, details\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser(description=\"Fetch GTEx expression (per-tissue median TPM) and compute overall stats.\")\n",
    "    ap.add_argument(\"--map\", dest=\"mapping\", required=True, help=\"Mapping CSV from step 1.\")\n",
    "    ap.add_argument(\"--out-prefix\", dest=\"out_prefix\", default=\"expr\", help=\"Output file prefix.\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    m = pd.read_csv(args.mapping)\n",
    "    req_cols = {\"input_protein\", \"symbol\", \"ensembl_gene\"}\n",
    "    if not req_cols.issubset(set(m.columns)):\n",
    "        raise ValueError(\"Mapping CSV must contain 'input_protein', 'symbol', 'ensembl_gene'.\")\n",
    "\n",
    "    # Deduplicate API calls by ensembl_gene\n",
    "    unique_ensg = sorted(set([str(x) for x in m[\"ensembl_gene\"].dropna().astype(str)]))\n",
    "    ensg_to_summary = {}\n",
    "    ensg_to_long = {}\n",
    "\n",
    "    for ensg in unique_ensg:\n",
    "        med, mean, details = gtex_expression_summary(ensg)\n",
    "        ensg_to_summary[ensg] = (med, mean)\n",
    "        ensg_to_long[ensg] = details\n",
    "        time.sleep(0.12)\n",
    "\n",
    "    # Build summary table (one row per input_protein)\n",
    "    sum_rows = []\n",
    "    for _, row in m.iterrows():\n",
    "        ip = row.get(\"input_protein\")\n",
    "        sym = row.get(\"symbol\")\n",
    "        ensg = row.get(\"ensembl_gene\")\n",
    "        med, mean = (None, None)\n",
    "        if pd.notna(ensg):\n",
    "            med, mean = ensg_to_summary.get(str(ensg), (None, None))\n",
    "        sum_rows.append({\n",
    "            \"input_protein\": ip,\n",
    "            \"symbol\": sym,\n",
    "            \"ensembl_gene\": ensg,\n",
    "            \"gtex_overall_median_tpm\": med,\n",
    "            \"gtex_overall_mean_tpm\": mean\n",
    "        })\n",
    "    summary_df = pd.DataFrame(sum_rows)\n",
    "\n",
    "    # Build long table (per tissue)\n",
    "    long_rows = []\n",
    "    for _, row in m.iterrows():\n",
    "        ip = row.get(\"input_protein\")\n",
    "        sym = row.get(\"symbol\")\n",
    "        ensg = row.get(\"ensembl_gene\")\n",
    "        details = []\n",
    "        if pd.notna(ensg):\n",
    "            details = ensg_to_long.get(str(ensg), [])\n",
    "        for d in details:\n",
    "            long_rows.append({\n",
    "                \"input_protein\": ip,\n",
    "                \"symbol\": sym,\n",
    "                \"ensembl_gene\": ensg,\n",
    "                \"tissue\": d.get(\"tissue\"),\n",
    "                \"median_tpm\": d.get(\"median_tpm\"),\n",
    "                \"unit\": d.get(\"unit\")\n",
    "            })\n",
    "    long_df = pd.DataFrame(long_rows)\n",
    "\n",
    "    summary_path = f\"{args.out_prefix}.summary.csv\"\n",
    "    long_path = f\"{args.out_prefix}.long.csv\"\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    long_df.to_csv(long_path, index=False)\n",
    "    print(f\"Done.\\nSummary: {summary_path}\\nLong: {long_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esslnc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
