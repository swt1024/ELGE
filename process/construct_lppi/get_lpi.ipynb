{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7500833",
   "metadata": {},
   "source": [
    "##### Step1 Filter the interaction data from NPInter and separately obtain the LPI for human and mouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb1bb5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99377/2111050260.py:36: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  human_lpi = filtered[df[10] == \"Homo sapiens\"].drop_duplicates()\n",
      "/tmp/ipykernel_99377/2111050260.py:37: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  mouse_lpi = filtered[df[10] == \"Mus musculus\"].drop_duplicates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Generated npinter_lpi.csv for human and mouse with columns: gene_name, gene_id, identifier, protein_name, uniprot_id, tissue_or_cellline.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = \"../../data/raw/lncRNA_interaction.txt\"\n",
    "output_human = \"../../data/LPI/human/npinter_lpi.csv\"\n",
    "output_mouse = \"../../data/LPI/mouse/npinter_lpi.csv\"\n",
    "\n",
    "# indices to keep: (gene_name, gene_id, protein_name, uniprot_id, tissue_or_cellline)\n",
    "keep_indices = [1, 2, 4, 5, 11]\n",
    "\n",
    "# final output header\n",
    "header = ['gene_name', 'gene_id', 'identifier', 'protein', 'uniprot_id', 'tissue_or_cellline']\n",
    "\n",
    "# read input file\n",
    "df = pd.read_csv(input_file, sep=\"\\t\", header=None, dtype=str)\n",
    "\n",
    "# filter rows: lncRNA - protein - binding\n",
    "df = df[(df[3] == \"lncRNA\") & (df[6] == \"protein\") & (df[13] == \"binding\")]\n",
    "\n",
    "# select needed columns\n",
    "filtered = df.iloc[:, keep_indices].copy()\n",
    "filtered.columns = ['gene_name', 'gene_id', 'protein', 'uniprot_id', 'tissue_or_cellline']\n",
    "\n",
    "# construct identifier column\n",
    "filtered[\"identifier\"] = filtered.apply(\n",
    "    lambda x: x[\"gene_id\"] if x[\"gene_id\"] != \"-\" else x[\"gene_name\"], axis=1\n",
    ")\n",
    "\n",
    "# reorder columns\n",
    "filtered = filtered[['gene_name', 'gene_id', 'identifier', 'protein', 'uniprot_id', 'tissue_or_cellline']]\n",
    "\n",
    "# remove self-loops (gene_name == protein)\n",
    "filtered = filtered[~((filtered['gene_name'] != \"\") & (filtered['protein'] != \"\") & \n",
    "                     (filtered['gene_name'] == filtered['protein']))]\n",
    "\n",
    "# split by species\n",
    "human_lpi = filtered[df[10] == \"Homo sapiens\"].drop_duplicates()\n",
    "mouse_lpi = filtered[df[10] == \"Mus musculus\"].drop_duplicates()\n",
    "\n",
    "# save to CSV\n",
    "human_lpi.to_csv(output_human, index=False, encoding='utf-8')\n",
    "mouse_lpi.to_csv(output_mouse, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Done. Generated npinter_lpi.csv for human and mouse with columns: \"\n",
    "      \"gene_name, gene_id, identifier, protein_name, uniprot_id, tissue_or_cellline.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa5e32",
   "metadata": {},
   "source": [
    "##### Step2：修复LPI数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd10cc",
   "metadata": {},
   "source": [
    "Step 2.1: 修正gene id列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa2cf5",
   "metadata": {},
   "source": [
    "Step 2.1.1 替换转录本id为对应的基因id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "064e7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "mapping_file_noncode6 = \"../../reference_lncRNA/human/transcript/NONCODEv6_human_hg38_lncRNA_trans.txt\"\n",
    "mapping_file_noncode5 = \"../../reference_lncRNA/human/transcript/NONCODEv5_human_hg38_lncRNA_trans.txt\"\n",
    "out_file = \"human_lpi_id_fixed.csv\"\n",
    "\n",
    "# Transcript-like ID prefixes to repair\n",
    "transcript_prefixes = (\"NONHSAT\",)\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: load transcript->gene mapping\n",
    "# Assumes each line has at least two columns:\n",
    "#   col0 = gene_id, col1 = transcript_id\n",
    "# Auto-detects delimiter (comma/tab/whitespace) via engine='python'.\n",
    "# -----------------------------\n",
    "def load_mapping(path: str) -> dict:\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        sep=None,                # auto-detect delimiter\n",
    "        engine=\"python\",\n",
    "        header=None,\n",
    "        usecols=[0, 1],          # [gene_id, transcript_id]\n",
    "        names=[\"gene_id\", \"transcript_id\"],\n",
    "        dtype=str\n",
    "    )\n",
    "    # normalize strings\n",
    "    df = df.dropna(subset=[\"gene_id\", \"transcript_id\"])\n",
    "    df[\"gene_id\"] = df[\"gene_id\"].str.strip()\n",
    "    df[\"transcript_id\"] = df[\"transcript_id\"].str.strip()\n",
    "    # build transcript -> gene mapping (v6/v5 priority handled outside)\n",
    "    return dict(zip(df[\"transcript_id\"], df[\"gene_id\"]))\n",
    "\n",
    "# Load mappings: v6 has higher priority than v5\n",
    "map_v6 = load_mapping(mapping_file_noncode6)\n",
    "map_v5 = load_mapping(mapping_file_noncode5)\n",
    "\n",
    "# -----------------------------\n",
    "# Load LPI table\n",
    "# Keep strings to avoid unintended type casting\n",
    "# -----------------------------\n",
    "\n",
    "# Normalize gene_id string for testing and lookups\n",
    "gid_norm = human_lpi[\"gene_id\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "# Identify rows that look like transcript IDs (to be repaired)\n",
    "mask_tx_like = gid_norm.str.startswith(transcript_prefixes)\n",
    "\n",
    "# Map transcript -> gene via v6 (priority) and v5 (fallback) only on those rows\n",
    "mapped_v6 = gid_norm.where(mask_tx_like).map(map_v6)\n",
    "mapped_v5 = gid_norm.where(mask_tx_like).map(map_v5)\n",
    "\n",
    "# Start with original gene_id and apply repairs\n",
    "new_gene_id = human_lpi[\"gene_id\"].copy()\n",
    "\n",
    "# Apply v6 where available\n",
    "mask_v6_hit = mapped_v6.notna()\n",
    "new_gene_id.loc[mask_v6_hit] = mapped_v6.loc[mask_v6_hit].values\n",
    "\n",
    "# Apply v5 where v6 missed but v5 hit\n",
    "mask_v5_hit = (~mask_v6_hit) & mapped_v5.notna()\n",
    "new_gene_id.loc[mask_v5_hit] = mapped_v5.loc[mask_v5_hit].values\n",
    "\n",
    "# Update gene_id in the dataframe\n",
    "human_lpi[\"gene_id\"] = new_gene_id\n",
    "\n",
    "# Rows actually repaired (either v6 or v5 hit)\n",
    "mask_repaired = mask_v6_hit | mask_v5_hit\n",
    "\n",
    "# Keep identifier in sync with repaired gene_id (same behavior as original script)\n",
    "human_lpi.loc[mask_repaired, \"identifier\"] = human_lpi.loc[mask_repaired, \"gene_id\"]\n",
    "\n",
    "# Save\n",
    "human_lpi.to_csv(out_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31a17f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mouse\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "mapping_file_noncode5 = \"../../reference_lncRNA/mouse/transcript/NONCODEv5_mouse_mm10_lncRNA_trans.txt\"\n",
    "out_file = \"mouse_lpi_id_fixed.csv\"\n",
    "\n",
    "transcript_prefixes = (\"NONMMUT\",)\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: load transcript->gene mapping\n",
    "# Assumes each line has at least two columns:\n",
    "#   col0 = gene_id, col1 = transcript_id\n",
    "# Auto-detects delimiter (comma/tab/whitespace) via engine='python'.\n",
    "# -----------------------------\n",
    "def load_mapping(path: str) -> dict:\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        sep=None,                # auto-detect delimiter\n",
    "        engine=\"python\",\n",
    "        header=None,\n",
    "        usecols=[0, 1],          # [gene_id, transcript_id]\n",
    "        names=[\"gene_id\", \"transcript_id\"],\n",
    "        dtype=str\n",
    "    )\n",
    "    # normalize strings\n",
    "    df = df.dropna(subset=[\"gene_id\", \"transcript_id\"])\n",
    "    df[\"gene_id\"] = df[\"gene_id\"].str.strip()\n",
    "    df[\"transcript_id\"] = df[\"transcript_id\"].str.strip()\n",
    "    # build transcript -> gene mapping (v6/v5 priority handled outside)\n",
    "    return dict(zip(df[\"transcript_id\"], df[\"gene_id\"]))\n",
    "\n",
    "# Load mappings\n",
    "map_v5 = load_mapping(mapping_file_noncode5)\n",
    "\n",
    "# -----------------------------\n",
    "# Load LPI table\n",
    "# Keep strings to avoid unintended type casting\n",
    "# -----------------------------\n",
    "\n",
    "# Normalize gene_id string for testing and lookups\n",
    "gid_norm = mouse_lpi[\"gene_id\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "# Identify rows that look like transcript IDs (to be repaired)\n",
    "mask_tx_like = gid_norm.str.startswith(transcript_prefixes)\n",
    "\n",
    "# Map transcript -> gene via v5\n",
    "mapped_v5 = gid_norm.where(mask_tx_like).map(map_v5)\n",
    "\n",
    "# Start with original gene_id and apply repairs\n",
    "new_gene_id = mouse_lpi[\"gene_id\"].copy()\n",
    "\n",
    "# Apply v5\n",
    "mask_v5_hit = mapped_v5.notna()\n",
    "new_gene_id.loc[mask_v5_hit] = mapped_v5.loc[mask_v5_hit].values\n",
    "\n",
    "# Update gene_id in the dataframe\n",
    "mouse_lpi[\"gene_id\"] = new_gene_id\n",
    "\n",
    "# Rows actually repaired\n",
    "mask_repaired = mask_v5_hit\n",
    "\n",
    "# Keep identifier in sync with repaired gene_id (same behavior as original script)\n",
    "mouse_lpi.loc[mask_repaired, \"identifier\"] = mouse_lpi.loc[mask_repaired, \"gene_id\"]\n",
    "\n",
    "# Save\n",
    "mouse_lpi.to_csv(out_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a189802",
   "metadata": {},
   "source": [
    "Step2.2 修正tissue_or_cellline列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f63e2",
   "metadata": {},
   "source": [
    "Step2.2.1 提取组织与细胞系信息并进行初步清洗，用于生成修正文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d51ba91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 769 -> saved to tissue_or_cellline_human_split.csv\n",
      "Mouse: 121 -> saved to tissue_or_cellline_mouse_split.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "SPLITERS = [\";\", \",\", \"and\"]\n",
    "SPLIT_RE = re.compile(r\"\\s*(?:;|,|\\band\\b)\\s*\", flags=re.IGNORECASE)\n",
    "PROTECTED_COMMA = \"§COMMA§\"\n",
    "\n",
    "def normalize_commas(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return str(text).replace(\"，\", \",\")\n",
    "\n",
    "def protect_commas_before_gestation_week(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    return re.sub(r\",(\\s*)(?=Gestation\\s+Week\\b)\", PROTECTED_COMMA + r\"\\1\", text, flags=re.IGNORECASE)\n",
    "\n",
    "def restore_protected_commas(text: str) -> str:\n",
    "    return text.replace(PROTECTED_COMMA, \",\")\n",
    "\n",
    "def smart_split(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split by ; , and \"and\", but ignore any of them that are inside (), [], or {}.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    buf = []\n",
    "    level = 0  # parenthesis nesting\n",
    "    i = 0\n",
    "    n = len(text)\n",
    "    while i < n:\n",
    "        ch = text[i]\n",
    "        # Handle entering/exiting parentheses\n",
    "        if ch in \"([{\":\n",
    "            level += 1\n",
    "            buf.append(ch)\n",
    "        elif ch in \")]}\":\n",
    "            level = max(level - 1, 0)\n",
    "            buf.append(ch)\n",
    "        elif level == 0:\n",
    "            # Only split at top-level\n",
    "            if text[i:i+3].lower() == \"and\" and \\\n",
    "                (i == 0 or not text[i-1].isalpha()) and \\\n",
    "                (i+3 == n or not text[i+3].isalpha()):\n",
    "                # Split at \"and\"\n",
    "                if buf:\n",
    "                    results.append(''.join(buf).strip())\n",
    "                    buf = []\n",
    "                i += 3\n",
    "                continue\n",
    "            elif ch in \";,\":\n",
    "                if buf:\n",
    "                    results.append(''.join(buf).strip())\n",
    "                    buf = []\n",
    "                i += 1\n",
    "                continue\n",
    "            else:\n",
    "                buf.append(ch)\n",
    "        else:\n",
    "            buf.append(ch)\n",
    "        i += 1\n",
    "    # Last buffer\n",
    "    if buf:\n",
    "        results.append(''.join(buf).strip())\n",
    "    # Clean empty\n",
    "    return [x for x in results if x]\n",
    "\n",
    "def split_cell_value(cell: str) -> List[str]:\n",
    "    if pd.isna(cell) or str(cell).strip() == \"\":\n",
    "        return []\n",
    "    s = normalize_commas(str(cell))\n",
    "    s = protect_commas_before_gestation_week(s)\n",
    "    # Use smart_split instead of SPLIT_RE\n",
    "    parts = smart_split(s)\n",
    "    tokens = []\n",
    "    for p in parts:\n",
    "        if not p:\n",
    "            continue\n",
    "        p = restore_protected_commas(p).strip().strip('\\\"“”\\'')\n",
    "        if p:\n",
    "            tokens.append(p)\n",
    "    return tokens\n",
    "\n",
    "def extract_split_unique(input_csv: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = input_csv[[\"tissue_or_cellline\"]]\n",
    "    contexts = (\n",
    "        df[\"tissue_or_cellline\"]\n",
    "        .map(split_cell_value)\n",
    "        .explode()\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "    )\n",
    "    out_df = (\n",
    "        pd.DataFrame({\"raw\": contexts})\n",
    "        .drop_duplicates()\n",
    "        .sort_values(\"raw\", kind=\"mergesort\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return out_df\n",
    "\n",
    "def main():\n",
    "\n",
    "    human_out = \"tissue_or_cellline_human_split.csv\"\n",
    "    mouse_out = \"tissue_or_cellline_mouse_split.csv\"\n",
    "\n",
    "    # ---- Human ----\n",
    "    human_ctx = extract_split_unique(human_lpi)\n",
    "    human_ctx.to_csv(human_out, index=False)\n",
    "\n",
    "    # ---- Mouse ----\n",
    "    mouse_ctx = extract_split_unique(mouse_lpi)\n",
    "    mouse_ctx.to_csv(mouse_out, index=False)\n",
    "\n",
    "    print(f\"Human: {len(human_ctx)} -> saved to {human_out}\")\n",
    "    print(f\"Mouse: {len(mouse_ctx)} -> saved to {mouse_out}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bfef17",
   "metadata": {},
   "source": [
    "Step2.2.2 修复tissue_or_cellline列，并将每一个组织或细胞系划分为一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c945cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human\n",
    "import pandas as pd\n",
    "import math\n",
    "from typing import List, Dict\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load data\n",
    "# -----------------------------\n",
    "#human_lpi = pd.read_csv(\"human_lpi_id_fixed.csv\")  # must contain column 'tissue_or_cellline'\n",
    "# Mapping file: first column = raw term to fix; second column = standardized name\n",
    "mapping_df = pd.read_csv(\"normalized_tissue_cellline_human.csv\", header=None, names=[\"raw\", \"std\"], dtype=str)\n",
    "# Build a case-insensitive mapping (trimmed and lowercased on the left key)\n",
    "mapping_dict: Dict[str, str] = { (r or \"\").strip().lower(): (s or \"\").strip() for r, s in mapping_df.values }\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Split function (ignore separators inside parentheses)\n",
    "# -----------------------------\n",
    "def split_outside_parentheses(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split by comma, semicolon, and the word 'and' while ignoring any separators inside parentheses.\n",
    "    - Treat 'and' as a word separator only when depth == 0 and flanked by word boundaries.\n",
    "    - Returns a list of raw tokens (not yet mapped).\n",
    "    \"\"\"\n",
    "    if text is None or (isinstance(text, float) and math.isnan(text)):\n",
    "        return []\n",
    "    s = str(text)\n",
    "\n",
    "    tokens = []\n",
    "    buf = []\n",
    "    depth = 0  # parentheses depth\n",
    "    i = 0\n",
    "    L = len(s)\n",
    "\n",
    "    def flush():\n",
    "        tok = \"\".join(buf).strip()\n",
    "        if tok:\n",
    "            tokens.append(tok)\n",
    "        buf.clear()\n",
    "\n",
    "    def is_word_boundary(ch: str) -> bool:\n",
    "        # Boundary if start/end or a non-alphanumeric char\n",
    "        return (not ch) or (not ch.isalnum())\n",
    "\n",
    "    while i < L:\n",
    "        ch = s[i]\n",
    "\n",
    "        # Track parentheses depth\n",
    "        if ch == '(':\n",
    "            depth += 1\n",
    "            buf.append(ch)\n",
    "            i += 1\n",
    "            continue\n",
    "        elif ch == ')':\n",
    "            depth = max(0, depth - 1)\n",
    "            buf.append(ch)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        if depth == 0:\n",
    "            # Check hard separators: comma or semicolon\n",
    "            if ch in {',', ';'}:\n",
    "                flush()\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # Check the word 'and' as a separator (case-insensitive) with word boundaries\n",
    "            if s[i:i+3].lower() == 'and':\n",
    "                prev_char = s[i-1] if i-1 >= 0 else \"\"\n",
    "                next_char = s[i+3] if i+3 < L else \"\"\n",
    "                if is_word_boundary(prev_char) and is_word_boundary(next_char):\n",
    "                    # finalize current token and skip 'and'\n",
    "                    flush()\n",
    "                    i += 3\n",
    "                    continue\n",
    "\n",
    "        # Default: keep character\n",
    "        buf.append(ch)\n",
    "        i += 1\n",
    "\n",
    "    # flush remainder\n",
    "    flush()\n",
    "\n",
    "    # Drop empty tokens after stripping\n",
    "    tokens = [t for t in (tok.strip() for tok in tokens) if t]\n",
    "    return tokens\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Normalize & map tokens\n",
    "# -----------------------------\n",
    "def map_token(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a raw token to standardized name using mapping_dict.\n",
    "    - Case-insensitive lookup (lowercased key).\n",
    "    - If not found, return stripped original token.\n",
    "    \"\"\"\n",
    "    if token is None:\n",
    "        return \"\"\n",
    "    key = token.strip().lower()\n",
    "    return mapping_dict.get(key, token.strip())\n",
    "\n",
    "def split_and_map_cellline_field(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split the field outside parentheses and apply mapping to each token.\n",
    "    Returns a list of standardized tokens.\n",
    "    \"\"\"\n",
    "    raw_tokens = split_outside_parentheses(text)\n",
    "    mapped = [map_token(t) for t in raw_tokens]\n",
    "    # Optionally de-duplicate while preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for m in mapped:\n",
    "        if m not in seen and m != \"\":\n",
    "            seen.add(m)\n",
    "            out.append(m)\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Explode rows: each token -> one row, other columns unchanged\n",
    "# -----------------------------\n",
    "# Create an intermediate list column with standardized tokens\n",
    "# Pre-clean: normalize fullwidth comma (U+FF0C) to ASCII comma\n",
    "human_lpi[\"tissue_or_cellline\"] = (\n",
    "    human_lpi[\"tissue_or_cellline\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"，\", \",\", regex=False)\n",
    ")\n",
    "\n",
    "human_lpi[\"_tissues_std\"] = human_lpi[\"tissue_or_cellline\"].apply(split_and_map_cellline_field)\n",
    "\n",
    "# Explode into multiple rows (one per standardized tissue/cell line)\n",
    "human_lpi = human_lpi.explode(\"_tissues_std\", ignore_index=True)\n",
    "\n",
    "# Replace original column\n",
    "human_lpi[\"tissue_or_cellline\"] = human_lpi[\"_tissues_std\"]\n",
    "human_lpi = human_lpi.drop(columns=[\"_tissues_std\"])\n",
    "\n",
    "# Optionally drop rows where the split produced no tokens\n",
    "human_lpi = human_lpi[human_lpi[\"tissue_or_cellline\"].notna() & (human_lpi[\"tissue_or_cellline\"].str.strip() != \"\")]\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Save\n",
    "# -----------------------------\n",
    "human_lpi.to_csv(\"human_lpi_tissue_fixed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7880abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mouse\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load data\n",
    "# -----------------------------\n",
    "#mouse_lpi = pd.read_csv(\"mouse_lpi_id_fixed.csv\")  # must contain column 'tissue_or_cellline'\n",
    "\n",
    "# Mapping file: first column = raw term to fix; second column = standardized name\n",
    "mapping_df = pd.read_csv(\"normalized_tissue_cellline_mouse.csv\", header=None, names=[\"raw\", \"std\"], dtype=str)\n",
    "# Build a case-insensitive mapping (trimmed and lowercased on the left key)\n",
    "mapping_dict: Dict[str, str] = { (r or \"\").strip().lower(): (s or \"\").strip() for r, s in mapping_df.values }\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Split function (ignore separators inside parentheses)\n",
    "# -----------------------------\n",
    "def split_outside_parentheses(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split by comma, semicolon, and the word 'and' while ignoring any separators inside parentheses.\n",
    "    - Treat 'and' as a word separator only when depth == 0 and flanked by word boundaries.\n",
    "    - Returns a list of raw tokens (not yet mapped).\n",
    "    \"\"\"\n",
    "    if text is None or (isinstance(text, float) and math.isnan(text)):\n",
    "        return []\n",
    "    s = str(text)\n",
    "\n",
    "    tokens = []\n",
    "    buf = []\n",
    "    depth = 0  # parentheses depth\n",
    "    i = 0\n",
    "    L = len(s)\n",
    "\n",
    "    def flush():\n",
    "        tok = \"\".join(buf).strip()\n",
    "        if tok:\n",
    "            tokens.append(tok)\n",
    "        buf.clear()\n",
    "\n",
    "    def is_word_boundary(ch: str) -> bool:\n",
    "        # Boundary if start/end or a non-alphanumeric char\n",
    "        return (not ch) or (not ch.isalnum())\n",
    "\n",
    "    while i < L:\n",
    "        ch = s[i]\n",
    "\n",
    "        # Track parentheses depth\n",
    "        if ch == '(':\n",
    "            depth += 1\n",
    "            buf.append(ch)\n",
    "            i += 1\n",
    "            continue\n",
    "        elif ch == ')':\n",
    "            depth = max(0, depth - 1)\n",
    "            buf.append(ch)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        if depth == 0:\n",
    "            # Check hard separators: comma or semicolon\n",
    "            if ch in {',', ';'}:\n",
    "                flush()\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # Check the word 'and' as a separator (case-insensitive) with word boundaries\n",
    "            if s[i:i+3].lower() == 'and':\n",
    "                prev_char = s[i-1] if i-1 >= 0 else \"\"\n",
    "                next_char = s[i+3] if i+3 < L else \"\"\n",
    "                if is_word_boundary(prev_char) and is_word_boundary(next_char):\n",
    "                    # finalize current token and skip 'and'\n",
    "                    flush()\n",
    "                    i += 3\n",
    "                    continue\n",
    "\n",
    "        # Default: keep character\n",
    "        buf.append(ch)\n",
    "        i += 1\n",
    "\n",
    "    # flush remainder\n",
    "    flush()\n",
    "\n",
    "    # Drop empty tokens after stripping\n",
    "    tokens = [t for t in (tok.strip() for tok in tokens) if t]\n",
    "    return tokens\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Normalize & map tokens\n",
    "# -----------------------------\n",
    "def map_token(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a raw token to standardized name using mapping_dict.\n",
    "    - Case-insensitive lookup (lowercased key).\n",
    "    - If not found, return stripped original token.\n",
    "    \"\"\"\n",
    "    if token is None:\n",
    "        return \"\"\n",
    "    key = token.strip().lower()\n",
    "    return mapping_dict.get(key, token.strip())\n",
    "\n",
    "def split_and_map_cellline_field(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split the field outside parentheses and apply mapping to each token.\n",
    "    Returns a list of standardized tokens.\n",
    "    \"\"\"\n",
    "    raw_tokens = split_outside_parentheses(text)\n",
    "    mapped = [map_token(t) for t in raw_tokens]\n",
    "    # Optionally de-duplicate while preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for m in mapped:\n",
    "        if m not in seen and m != \"\":\n",
    "            seen.add(m)\n",
    "            out.append(m)\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Explode rows: each token -> one row, other columns unchanged\n",
    "# -----------------------------\n",
    "# Create an intermediate list column with standardized tokens\n",
    "# Pre-clean: normalize fullwidth comma (U+FF0C) to ASCII comma\n",
    "mouse_lpi[\"tissue_or_cellline\"] = (\n",
    "    mouse_lpi[\"tissue_or_cellline\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"，\", \",\", regex=False)\n",
    ")\n",
    "mouse_lpi[\"_tissues_std\"] = mouse_lpi[\"tissue_or_cellline\"].apply(split_and_map_cellline_field)\n",
    "\n",
    "# Explode into multiple rows (one per standardized tissue/cell line)\n",
    "mouse_lpi = mouse_lpi.explode(\"_tissues_std\", ignore_index=True)\n",
    "\n",
    "# Replace original column\n",
    "mouse_lpi[\"tissue_or_cellline\"] = mouse_lpi[\"_tissues_std\"]\n",
    "mouse_lpi = mouse_lpi.drop(columns=[\"_tissues_std\"])\n",
    "\n",
    "# Optionally drop rows where the split produced no tokens\n",
    "mouse_lpi = mouse_lpi[mouse_lpi[\"tissue_or_cellline\"].notna() & (mouse_lpi[\"tissue_or_cellline\"].str.strip() != \"\")]\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Save\n",
    "# -----------------------------\n",
    "#mouse_lpi.to_csv(\"mouse_lpi_tissue.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abf29a",
   "metadata": {},
   "source": [
    "Step2.3 Replace invalid identifier with gene_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ----------------------------\n",
    "# Paths (adjust as needed)\n",
    "# ----------------------------\n",
    "ensembl_dir = \"../../reference_lncRNA/human/bed/ensembl/\"\n",
    "# human_lpi = pd.read_csv('human_lpi_tissue_fixed.csv')\n",
    "\n",
    "# ----------------------------\n",
    "# Load NONCODE gene_id lists (only gene_id column is needed)\n",
    "# NONCODE BED columns: chr, start, end, gene_id, score, strand\n",
    "# ----------------------------\n",
    "noncodev5_ids = pd.read_csv(\n",
    "    '../../reference_lncRNA/human/bed/NONCODEv5_hg38.lncRNAGene.bed',\n",
    "    sep='\\t', header=None, usecols=[3], names=['gene_id']\n",
    ")['gene_id']\n",
    "\n",
    "noncodev6_ids = pd.read_csv(\n",
    "    '../../reference_lncRNA/human/bed/NONCODEv6_hg38.lncRNAGene.bed',\n",
    "    sep='\\t', header=None, usecols=[3], names=['gene_id']\n",
    ")['gene_id']\n",
    "\n",
    "# ----------------------------\n",
    "# Collect Ensembl gene_id from all BED files in the directory\n",
    "# Ensembl BED columns: chr, start, end, gene_name, gene_id, strand\n",
    "# ----------------------------\n",
    "def extract_version(filename: str) -> int:\n",
    "    \"\"\"\n",
    "    Extract Ensembl GRCh38 version number from filename (e.g., '...GRCh38.<n>.bed').\n",
    "    Returns -1 if not matched.\n",
    "    \"\"\"\n",
    "    m = re.search(r'GRCh38\\.(\\d+)\\.bed', filename)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "bed_files = [f for f in os.listdir(ensembl_dir) if f.endswith(\".bed\")]\n",
    "# Sorting not strictly required for validity checking, but kept for consistency\n",
    "bed_files_sorted = sorted(bed_files, key=extract_version, reverse=True)\n",
    "\n",
    "ensembl_ids_list = []\n",
    "for bed_file in bed_files_sorted:\n",
    "    bed_path = os.path.join(ensembl_dir, bed_file)\n",
    "    # Read only the gene_id column (index 4)\n",
    "    gid = pd.read_csv(bed_path, sep='\\t', header=None, usecols=[4], names=['gene_id'])['gene_id']\n",
    "    ensembl_ids_list.append(gid)\n",
    "\n",
    "ensembl_ids = pd.concat(ensembl_ids_list, ignore_index=True) if ensembl_ids_list else pd.Series([], dtype=object)\n",
    "\n",
    "# ----------------------------\n",
    "# Build a set of valid gene IDs across all sources\n",
    "# ----------------------------\n",
    "def to_id_set(series: pd.Series) -> set:\n",
    "    \"\"\"\n",
    "    Normalize a Series to a set of non-empty string IDs:\n",
    "    - drop NaN\n",
    "    - strip spaces\n",
    "    - drop empty strings\n",
    "    \"\"\"\n",
    "    s = series.dropna().astype(str).str.strip()\n",
    "    s = s[s != \"\"]\n",
    "    return set(s)\n",
    "\n",
    "valid_ids = to_id_set(noncodev6_ids) | to_id_set(noncodev5_ids) | to_id_set(ensembl_ids)\n",
    "\n",
    "# ----------------------------\n",
    "# Determine invalid IDs and update 'identifier' accordingly\n",
    "# Rules:\n",
    "# - If gene_id is NOT in valid_ids (or is null/empty) -> treat as invalid\n",
    "# - For invalid rows: identifier := gene_name (gene_name!=-,gene_name is valid)\n",
    "# - For valid rows: identifier remains unchanged\n",
    "# - Filter out rows with invalid gene_id and invalid gene_name\n",
    "# ----------------------------\n",
    "# ----------------------------\n",
    "# Determine valid and invalid gene_id\n",
    "# ----------------------------\n",
    "gene_id_raw = human_lpi['gene_id']\n",
    "gene_id_norm = gene_id_raw.astype(str).str.strip()\n",
    "\n",
    "mask_valid_id = gene_id_norm.isin(valid_ids)\n",
    "mask_invalid_id = gene_id_raw.isna() | (gene_id_norm == \"\") | (~gene_id_norm.isin(valid_ids))\n",
    "\n",
    "# ----------------------------\n",
    "# For invalid gene_id, check gene_name validity\n",
    "# ----------------------------\n",
    "gene_name_norm = human_lpi['gene_name'].astype(str).str.strip()\n",
    "mask_valid_name = (gene_name_norm != \"\") & (gene_name_norm != \"-\")\n",
    "\n",
    "# Rows to replace identifier with gene_name\n",
    "mask_replace = mask_invalid_id & mask_valid_name\n",
    "\n",
    "# Rows to drop: both ID invalid and name invalid\n",
    "mask_drop = mask_invalid_id & (~mask_valid_name)\n",
    "\n",
    "# Apply replacement\n",
    "human_lpi.loc[mask_replace, 'identifier'] = human_lpi.loc[mask_replace, 'gene_name']\n",
    "\n",
    "# Drop completely invalid rows\n",
    "human_lpi = human_lpi.loc[~mask_drop].copy()\n",
    "\n",
    "# save the updated table\n",
    "human_lpi.to_csv('./human_lpi_fixed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mouse\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ----------------------------\n",
    "# Paths (adjust as needed)\n",
    "# ----------------------------\n",
    "ensembl_dir = \"../../reference_lncRNA/mouse/bed/ensembl/\"\n",
    "#mouse_lpi = pd.read_csv('mouse_lpi_tissue_fixed.csv')\n",
    "\n",
    "# ----------------------------\n",
    "# Load NONCODE gene_id lists (only gene_id column is needed)\n",
    "# NONCODE BED columns: chr, start, end, gene_id, score, strand\n",
    "# ----------------------------\n",
    "noncodev5_ids = pd.read_csv(\n",
    "    '../../reference_lncRNA/mouse/bed/NONCODEv5_mm10.lncRNAGene.bed',\n",
    "    sep='\\t', header=None, usecols=[3], names=['gene_id']\n",
    ")['gene_id']\n",
    "\n",
    "noncodev6_ids = pd.read_csv(\n",
    "    '../../reference_lncRNA/mouse/bed/NONCODEv6_mm10.lncRNAGene.bed',\n",
    "    sep='\\t', header=None, usecols=[3], names=['gene_id']\n",
    ")['gene_id']\n",
    "\n",
    "# ----------------------------\n",
    "# Collect Ensembl gene_id from all BED files in the directory\n",
    "# Ensembl BED columns: chr, start, end, gene_name, gene_id, strand\n",
    "# ----------------------------\n",
    "def extract_version(filename: str) -> int:\n",
    "    \"\"\"\n",
    "    Extract Ensembl GRCm38 version number from filename (e.g., '...GRCm38.<n>.bed').\n",
    "    Returns -1 if not matched.\n",
    "    \"\"\"\n",
    "    m = re.search(r'GRCm38\\.(\\d+)\\.bed', filename)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "bed_files = [f for f in os.listdir(ensembl_dir) if f.endswith(\".bed\")]\n",
    "# Sorting not strictly required for validity checking, but kept for consistency\n",
    "bed_files_sorted = sorted(bed_files, key=extract_version, reverse=True)\n",
    "\n",
    "ensembl_ids_list = []\n",
    "for bed_file in bed_files_sorted:\n",
    "    bed_path = os.path.join(ensembl_dir, bed_file)\n",
    "    # Read only the gene_id column (index 4)\n",
    "    gid = pd.read_csv(bed_path, sep='\\t', header=None, usecols=[4], names=['gene_id'])['gene_id']\n",
    "    ensembl_ids_list.append(gid)\n",
    "\n",
    "ensembl_ids = pd.concat(ensembl_ids_list, ignore_index=True) if ensembl_ids_list else pd.Series([], dtype=object)\n",
    "\n",
    "# ----------------------------\n",
    "# Build a set of valid gene IDs across all sources\n",
    "# ----------------------------\n",
    "def to_id_set(series: pd.Series) -> set:\n",
    "    \"\"\"\n",
    "    Normalize a Series to a set of non-empty string IDs:\n",
    "    - drop NaN\n",
    "    - strip spaces\n",
    "    - drop empty strings\n",
    "    \"\"\"\n",
    "    s = series.dropna().astype(str).str.strip()\n",
    "    s = s[s != \"\"]\n",
    "    return set(s)\n",
    "\n",
    "valid_ids = to_id_set(noncodev6_ids) | to_id_set(noncodev5_ids) | to_id_set(ensembl_ids)\n",
    "\n",
    "# ----------------------------\n",
    "# Determine invalid IDs and update 'identifier' accordingly\n",
    "# Rules:\n",
    "# - If gene_id is NOT in valid_ids (or is null/empty) -> treat as invalid\n",
    "# - For invalid rows: identifier := gene_name (gene_name!=-,gene_name is valid)\n",
    "# - For valid rows: identifier remains unchanged\n",
    "# - Filter out rows with invalid gene_id and invalid gene_name\n",
    "# ----------------------------\n",
    "# ----------------------------\n",
    "# Determine valid and invalid gene_id\n",
    "# ----------------------------\n",
    "gene_id_raw = mouse_lpi['gene_id']\n",
    "gene_id_norm = gene_id_raw.astype(str).str.strip()\n",
    "\n",
    "mask_valid_id = gene_id_norm.isin(valid_ids)\n",
    "mask_invalid_id = gene_id_raw.isna() | (gene_id_norm == \"\") | (~gene_id_norm.isin(valid_ids))\n",
    "\n",
    "# ----------------------------\n",
    "# For invalid gene_id, check gene_name validity\n",
    "# ----------------------------\n",
    "gene_name_norm = mouse_lpi['gene_name'].astype(str).str.strip()\n",
    "mask_valid_name = (gene_name_norm != \"\") & (gene_name_norm != \"-\")\n",
    "\n",
    "# Rows to replace identifier with gene_name\n",
    "mask_replace = mask_invalid_id & mask_valid_name\n",
    "\n",
    "# Rows to drop: both ID invalid and name invalid\n",
    "mask_drop = mask_invalid_id & (~mask_valid_name)\n",
    "\n",
    "# Apply replacement\n",
    "mouse_lpi.loc[mask_replace, 'identifier'] = mouse_lpi.loc[mask_replace, 'gene_name']\n",
    "\n",
    "# save the updated table\n",
    "mouse_lpi.to_csv('./mouse_lpi_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b0b5f7",
   "metadata": {},
   "source": [
    "##### 构建LPI的无向有权图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9af2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human\n",
    "\n",
    "human_lpi = pd.read_csv(\"human_lpi_fixed.csv\")\n",
    "\n",
    "human_lpi['lncRNA_id'] = [\"l\" + str(x) for x in human_lpi['identifier']]\n",
    "human_lpi[\"gene_name\"] = (\n",
    "    human_lpi[\"gene_name\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"‐\", \"-\", regex=False)\n",
    ")\n",
    "\n",
    "def concat_ignore_dash(series):\n",
    "    vals = [str(v).strip() for v in series if pd.notna(v) and str(v).strip() != \"-\"]\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for v in vals:\n",
    "        if v not in seen:\n",
    "            seen.add(v)\n",
    "            out.append(v)\n",
    "    return \";\".join(out)\n",
    "\n",
    "lncRNA = human_lpi[['identifier','gene_name','gene_id','lncRNA_id']].drop_duplicates()\n",
    "lncRNA = (\n",
    "    lncRNA.groupby([\"lncRNA_id\",\"identifier\"], as_index=False)\n",
    "      .agg({\n",
    "          \"gene_name\": concat_ignore_dash,\n",
    "          \"gene_id\": concat_ignore_dash,\n",
    "      })\n",
    ")\n",
    "lncRNA.to_csv(\"../../data/LPI/human/lncRNA.csv\", index=False)\n",
    "\n",
    "protein = human_lpi[['protein', 'uniprot_id']].drop_duplicates()\n",
    "protein.to_csv(\"../../data/LPI/human/protein.csv\", index=False)\n",
    "\n",
    "human_lpi = human_lpi.drop(['gene_id', 'gene_name', 'identifier', 'uniprot_id'], axis=1)\n",
    "\n",
    "human_lpi_weighted = (\n",
    "    human_lpi\n",
    "    .groupby([\"lncRNA_id\", \"protein\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"edge_weight\")\n",
    ")\n",
    "human_lpi_weighted.to_csv(\"../../data/LPI/human/lpi_weighted.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ea968",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# Mouse\n",
    "\n",
    "mouse_lpi = pd.read_csv(\"mouse_lpi_fixed.csv\")\n",
    "\n",
    "mouse_lpi['lncRNA_id'] = [\"l\" + str(x) for x in mouse_lpi['identifier']]\n",
    "mouse_lpi[\"gene_name\"] = (\n",
    "    mouse_lpi[\"gene_name\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"‐\", \"-\", regex=False)\n",
    ")\n",
    "\n",
    "def concat_ignore_dash(series):\n",
    "    vals = [str(v).strip() for v in series if pd.notna(v) and str(v).strip() != \"-\"]\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for v in vals:\n",
    "        if v not in seen:\n",
    "            seen.add(v)\n",
    "            out.append(v)\n",
    "    return \";\".join(out)\n",
    "\n",
    "lncRNA = mouse_lpi[['identifier', 'gene_name','gene_id','lncRNA_id']].drop_duplicates()\n",
    "lncRNA = (\n",
    "    lncRNA.groupby([\"lncRNA_id\",\"identifier\"], as_index=False)\n",
    "      .agg({\n",
    "          \"gene_name\": concat_ignore_dash,\n",
    "          \"gene_id\": concat_ignore_dash\n",
    "      })\n",
    ")\n",
    "lncRNA.to_csv(\"../../data/LPI/mouse/lncRNA.csv\", index=False)\n",
    "\n",
    "protein = mouse_lpi[['protein', 'uniprot_id']].drop_duplicates()\n",
    "protein.to_csv(\"../../data/LPI/mouse/protein.csv\", index=False)\n",
    "\n",
    "mouse_lpi = mouse_lpi.drop(['gene_id', 'gene_name', 'identifier', 'uniprot_id'], axis=1)\n",
    "\n",
    "mouse_lpi_weighted = (\n",
    "    mouse_lpi\n",
    "    .groupby([\"lncRNA_id\", \"protein\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"edge_weight\")\n",
    ")\n",
    "mouse_lpi_weighted.to_csv(\"../../data/LPI/mouse/lpi_weighted.csv\",index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esslnc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
