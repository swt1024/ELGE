{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "522fa935",
   "metadata": {},
   "source": [
    "#### Filter the BioGRID data to obtain human and mouse PPI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf5d2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Human PPI edges: ../../data/PPI/human/ppi.csv\n",
      "[DONE] Human proteins in PPI: ../../data/PPI/human/protein_in_ppi.csv\n"
     ]
    }
   ],
   "source": [
    "# Human\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ======================= paths ======================\n",
    "human_PPI_file = '../../data/raw/BIOGRID-ORGANISM-Homo_sapiens-4.4.248.tab3.txt'\n",
    "human_edges_out = '../../data/PPI/human/ppi.csv'\n",
    "human_proteins_out = '../../data/PPI/human/protein_in_ppi.csv'\n",
    "# ====================================================\n",
    "\n",
    "# Read TAB3 as strings\n",
    "df = pd.read_csv(human_PPI_file, sep='\\t', dtype=str, low_memory=False)\n",
    "\n",
    "# --- (1) Keep physical interactions only ---\n",
    "# Experimental System Type column should be 'physical' (case-insensitive)\n",
    "df = df[df['Experimental System Type'].str.lower() == 'physical']\n",
    "\n",
    "# --- (2) Keep only rows where BOTH interactors are proteins ---\n",
    "# In TAB3, UniProt evidence exists if Swiss-Prot or TrEMBL accession is present (not '-')\n",
    "a_has_protein = (df['SWISS-PROT Accessions Interactor A'].fillna('-') != '-') | \\\n",
    "                (df['TREMBL Accessions Interactor A'].fillna('-') != '-')\n",
    "b_has_protein = (df['SWISS-PROT Accessions Interactor B'].fillna('-') != '-') | \\\n",
    "                (df['TREMBL Accessions Interactor B'].fillna('-') != '-')\n",
    "df = df[a_has_protein & b_has_protein]\n",
    "\n",
    "# --- (3) Keep species = human-human ---\n",
    "df = df[\n",
    "    (df['Organism Name Interactor A'] == 'Homo sapiens') &\n",
    "    (df['Organism Name Interactor B'] == 'Homo sapiens')\n",
    "]\n",
    "\n",
    "# --- (4) Pick readable symbols as endpoints ---\n",
    "edges = df[['Official Symbol Interactor A', 'Official Symbol Interactor B']].copy()\n",
    "edges.columns = ['Protein A', 'Protein B']\n",
    "\n",
    "# --- (5) Remove self-loops ---\n",
    "edges = edges[edges['Protein A'] != edges['Protein B']]\n",
    "\n",
    "# --- (6) Save PPI edges (symbol-symbol) ---\n",
    "edges.to_csv(human_edges_out, index=False)\n",
    "\n",
    "# --- (7) Build unique protein list observed in PPI and save ---\n",
    "proteins = pd.DataFrame({\n",
    "    'protein': pd.concat([edges['Protein A'], edges['Protein B']], ignore_index=True).astype(str).str.strip()\n",
    "})\n",
    "proteins.drop_duplicates().to_csv(human_proteins_out, index=False)\n",
    "\n",
    "print(f\"[DONE] Human PPI edges: {human_edges_out}\")\n",
    "print(f\"[DONE] Human proteins in PPI: {human_proteins_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb769ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Mouse PPI edges: ../../data/PPI/mouse/ppi.csv\n",
      "[DONE] Mouse proteins in PPI: ../../data/PPI/mouse/protein_in_ppi.csv\n"
     ]
    }
   ],
   "source": [
    "# Mouse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  # If NumPy is not installed, replace with pandas-only approach below\n",
    "from pathlib import Path\n",
    "\n",
    "# ==================== User paths ====================\n",
    "mouse_PPI_file = '../../data/raw/BIOGRID-ORGANISM-Mus_musculus-4.4.248.tab3.txt'\n",
    "mouse_edges_out = '../../data/PPI/mouse/ppi.csv'\n",
    "mouse_proteins_out = '../../data/PPI/mouse/protein_in_ppi.csv'\n",
    "# ====================================================\n",
    "\n",
    "# Read TAB3 as strings\n",
    "df = pd.read_csv(mouse_PPI_file, sep='\\t', dtype=str, low_memory=False)\n",
    "\n",
    "# --- (1) Keep physical interactions only ---\n",
    "df = df[df['Experimental System Type'].str.lower() == 'physical']\n",
    "\n",
    "# --- (2) Keep only rows where BOTH interactors are proteins (Swiss-Prot or TrEMBL present) ---\n",
    "a_has_protein = (df['SWISS-PROT Accessions Interactor A'].fillna('-') != '-') | \\\n",
    "                (df['TREMBL Accessions Interactor A'].fillna('-') != '-')\n",
    "b_has_protein = (df['SWISS-PROT Accessions Interactor B'].fillna('-') != '-') | \\\n",
    "                (df['TREMBL Accessions Interactor B'].fillna('-') != '-')\n",
    "df = df[a_has_protein & b_has_protein]\n",
    "\n",
    "# --- (3) Keep species = mouse-mouse ---\n",
    "df = df[\n",
    "    (df['Organism Name Interactor A'] == 'Mus musculus') &\n",
    "    (df['Organism Name Interactor B'] == 'Mus musculus')\n",
    "]\n",
    "\n",
    "# --- (4) Build symbol-symbol edges ---\n",
    "edges = df[['Official Symbol Interactor A', 'Official Symbol Interactor B']].copy()\n",
    "edges.columns = ['Protein A', 'Protein B']\n",
    "\n",
    "# --- (5) Remove self-loops ---\n",
    "edges = edges[edges['Protein A'] != edges['Protein B']]\n",
    "\n",
    "# --- (6) Save PPI edges ---\n",
    "edges.to_csv(mouse_edges_out, index=False)\n",
    "\n",
    "# --- (7) Save unique proteins ---\n",
    "proteins = pd.DataFrame({\n",
    "    'protein': pd.concat([edges['Protein A'], edges['Protein B']], ignore_index=True).astype(str).str.strip()\n",
    "})\n",
    "proteins.drop_duplicates().to_csv(mouse_proteins_out, index=False)\n",
    "\n",
    "print(f\"[DONE] Mouse PPI edges: {mouse_edges_out}\")\n",
    "print(f\"[DONE] Mouse proteins in PPI: {mouse_proteins_out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e8cb28",
   "metadata": {},
   "source": [
    "##### Construct undirected weighted graph for the PPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e7f3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted PPI network saved to ../../data/PPI/human/ppi_weighted.csv\n"
     ]
    }
   ],
   "source": [
    "# Human\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"../../data/PPI/human/ppi.csv\"   # PPI file\n",
    "output_file = \"../../data/PPI/human/ppi_weighted.csv\"\n",
    "\n",
    "# Load the data (two columns: Protein A, Protein B)\n",
    "ppi = pd.read_csv(input_file)\n",
    "\n",
    "# Normalize edges for undirected graph: sort the two nodes so (A,B) == (B,A)\n",
    "ppi['protein_a'] = ppi[['Protein A', 'Protein B']].min(axis=1)\n",
    "ppi['protein_b'] = ppi[['Protein A', 'Protein B']].max(axis=1)\n",
    "\n",
    "# Count occurrences of each undirected edge\n",
    "edge_weights = (\n",
    "    ppi.groupby(['protein_a', 'protein_b'])\n",
    "       .size()\n",
    "       .reset_index(name='edge_weight')\n",
    ")\n",
    "\n",
    "# Save result\n",
    "edge_weights.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Weighted PPI network saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69806695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted PPI network saved to ../../data/PPI/mouse/ppi_weighted.csv\n"
     ]
    }
   ],
   "source": [
    "# Mouse\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"../../data/PPI/mouse/ppi.csv\"   # PPI file\n",
    "output_file = \"../../data/PPI/mouse/ppi_weighted.csv\"\n",
    "\n",
    "# Load the data (two columns: Protein A, Protein B)\n",
    "ppi = pd.read_csv(input_file)\n",
    "\n",
    "# Normalize edges for undirected graph: sort the two nodes so (A,B) == (B,A)\n",
    "ppi['protein_a'] = ppi[['Protein A', 'Protein B']].min(axis=1)\n",
    "ppi['protein_b'] = ppi[['Protein A', 'Protein B']].max(axis=1)\n",
    "\n",
    "# Count occurrences of each undirected edge\n",
    "edge_weights = (\n",
    "    ppi.groupby(['protein_a', 'protein_b'])\n",
    "       .size()\n",
    "       .reset_index(name='edge_weight')\n",
    ")\n",
    "\n",
    "# Save result\n",
    "edge_weights.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Weighted PPI network saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857733c9",
   "metadata": {},
   "source": [
    "#### Merge the PPI and LPI networks to generate the LPPI network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19cad718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete! Final LPPI file saved to ../../data/LPPI/human/lppi.csv\n",
      "Unified protein mapping saved to ../../data/LPPI/human/protein.csv\n"
     ]
    }
   ],
   "source": [
    "# Human\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "lpi_file = '../../data/LPI/human/lpi_weighted.csv'       # LPI file (contains lncRNA-protein interactions)\n",
    "lpi_protein_file = '../../data/LPI/human/protein.csv'    # LPI protein file (must contain 'protein' column)\n",
    "ppi_file = '../../data/PPI/human/ppi_weighted.csv'                # PPI file (must contain 'Protein A', 'Protein B')\n",
    "ppi_protein_file = '../../data/PPI/human/protein_in_ppi.csv'\n",
    "output_file = '../../data/LPPI/human/lppi.csv'\n",
    "\n",
    "# Load input data\n",
    "lpi_data = pd.read_csv(lpi_file)\n",
    "lpi_proteins = pd.read_csv(lpi_protein_file)\n",
    "ppi_data = pd.read_csv(ppi_file)\n",
    "ppi_proteins = pd.read_csv(ppi_protein_file)\n",
    "\n",
    "# --- 1) Collect all unique protein names from PPI and LPI ---\n",
    "lpi_proteins = lpi_proteins[['protein']]\n",
    "all_proteins = pd.concat([ppi_proteins, lpi_proteins], ignore_index=True).dropna()\n",
    "all_proteins = all_proteins[all_proteins != \"\"].drop_duplicates()\n",
    "\n",
    "# --- 2) Assign IDs in the format \"p<name>\" ---\n",
    "all_proteins['protein_id'] = [\"p\" + str(x) for x in all_proteins['protein']]\n",
    "\n",
    "# Save unified protein mapping\n",
    "all_proteins.to_csv('../../data/LPPI/human/protein.csv', index=False)\n",
    "\n",
    "protein_map = all_proteins[['protein','protein_id']]\n",
    "# --- 3) Map new IDs back to PPI ---\n",
    "ppi_data = ppi_data.merge(protein_map, left_on='protein_a', right_on='protein', how='left')\n",
    "ppi_data = ppi_data.rename(columns={'protein_id': 'Node_i'}).drop(columns=['protein','protein_a'])\n",
    "\n",
    "ppi_data = ppi_data.merge(protein_map, left_on='protein_b', right_on='protein', how='left')\n",
    "ppi_data = ppi_data.rename(columns={'protein_id': 'Node_j'}).drop(columns=['protein','protein_b'])\n",
    "\n",
    "# --- 4) Map new IDs back to LPI ---\n",
    "lpi_data = lpi_data.merge(protein_map, on='protein', how='left')\n",
    "\n",
    "# --- 5) Build edge tables in format: Node_i, Node_j ---\n",
    "lpi_data = lpi_data[['lncRNA_id', 'protein_id', 'edge_weight']].rename(columns={'lncRNA_id': 'Node_i', 'protein_id': 'Node_j'})\n",
    "\n",
    "# --- 6) Merge LPI and PPI edges into final LPPI file ---\n",
    "lppi = pd.concat([lpi_data, ppi_data], ignore_index=True)\n",
    "lppi.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Processing complete! Final LPPI file saved to {output_file}\")\n",
    "print(\"Unified protein mapping saved to ../../data/LPPI/human/protein.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54f70706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete! Final LPPI file saved to ../../data/LPPI/mouse/lppi.csv\n",
      "Unified protein mapping saved to ../../data/LPPI/mouse/protein.csv\n"
     ]
    }
   ],
   "source": [
    "# Mouse\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "lpi_file = '../../data/LPI/mouse/lpi_weighted.csv'       # LPI file (contains lncRNA-protein interactions)\n",
    "lpi_protein_file = '../../data/LPI/mouse/protein.csv'    # LPI protein file (must contain 'protein' column)\n",
    "ppi_file = '../../data/PPI/mouse/ppi_weighted.csv'                # PPI file (must contain 'Protein A', 'Protein B')\n",
    "ppi_protein_file = '../../data/PPI/mouse/protein_in_ppi.csv'\n",
    "output_file = '../../data/LPPI/mouse/lppi.csv'\n",
    "\n",
    "# Load input data\n",
    "lpi_data = pd.read_csv(lpi_file)\n",
    "lpi_proteins = pd.read_csv(lpi_protein_file)\n",
    "ppi_data = pd.read_csv(ppi_file)\n",
    "ppi_proteins = pd.read_csv(ppi_protein_file)\n",
    "\n",
    "# --- 1) Collect all unique protein names from PPI and LPI ---\n",
    "lpi_proteins = lpi_proteins[['protein']]\n",
    "all_proteins = pd.concat([ppi_proteins, lpi_proteins], ignore_index=True).dropna()\n",
    "all_proteins = all_proteins[all_proteins != \"\"].drop_duplicates()\n",
    "\n",
    "# --- 2) Assign IDs in the format \"p<name>\" ---\n",
    "all_proteins['protein_id'] = [\"p\" + str(x) for x in all_proteins['protein']]\n",
    "\n",
    "# Save unified protein mapping\n",
    "all_proteins.to_csv('../../data/LPPI/mouse/protein.csv', index=False)\n",
    "\n",
    "protein_map = all_proteins[['protein','protein_id']]\n",
    "\n",
    "# --- 3) Map new IDs back to PPI ---\n",
    "ppi_data = ppi_data.merge(protein_map, left_on='protein_a', right_on='protein', how='left')\n",
    "ppi_data = ppi_data.rename(columns={'protein_id': 'Node_i'}).drop(columns=['protein','protein_a'])\n",
    "\n",
    "ppi_data = ppi_data.merge(protein_map, left_on='protein_b', right_on='protein', how='left')\n",
    "ppi_data = ppi_data.rename(columns={'protein_id': 'Node_j'}).drop(columns=['protein','protein_b'])\n",
    "\n",
    "# --- 4) Map new IDs back to LPI ---\n",
    "lpi_data = lpi_data.merge(protein_map, on='protein', how='left')\n",
    "\n",
    "# --- 5) Build edge tables in format: Node_i, Node_j ---\n",
    "lpi_data = lpi_data[['lncRNA_id', 'protein_id', 'edge_weight']].rename(columns={'lncRNA_id': 'Node_i', 'protein_id': 'Node_j'})\n",
    "\n",
    "# --- 6) Merge LPI and PPI edges into final LPPI file ---\n",
    "lppi = pd.concat([lpi_data, ppi_data], ignore_index=True)\n",
    "lppi.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Processing complete! Final LPPI file saved to {output_file}\")\n",
    "print(\"Unified protein mapping saved to ../../data/LPPI/mouse/protein.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17192428",
   "metadata": {},
   "source": [
    "#### Fix protein name\n",
    "right_human_protein.csv and right_mouse_protein.csv are used to correct protein name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "388414c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: protein_updated.csv\n",
      "Saved: lppi_updated.csv (includes LPI + PPI)\n"
     ]
    }
   ],
   "source": [
    "# Human\n",
    "import pandas as pd\n",
    "\n",
    "# === Configuration file paths ===\n",
    "mapping_path = \"right_human_protein.csv\"         \n",
    "protein_path = \"../../data/LPPI/human/protein.csv\"        \n",
    "lppi_path = \"../../data/LPPI/human/lppi.csv\"               \n",
    "\n",
    "# === Read mapping table ===\n",
    "df_map = pd.read_csv(mapping_path, header=None, names=[\"raw\", \"std\"])\n",
    "symbol_map = dict(zip(df_map[\"raw\"], df_map[\"std\"]))\n",
    "\n",
    "# === Process protein file ===\n",
    "df_protein = pd.read_csv(protein_path)\n",
    "df_protein[\"NewSymbol\"] = df_protein[\"protein\"].map(symbol_map).fillna(df_protein[\"protein\"])\n",
    "df_protein[\"NewProteinID\"] = \"p\" + df_protein[\"NewSymbol\"]\n",
    "\n",
    "# Save the updated protein file\n",
    "update_protein = df_protein[[\"NewSymbol\", \"NewProteinID\"]]\n",
    "update_protein.columns = ['protein', 'protein_id']\n",
    "update_protein = update_protein.drop_duplicates()\n",
    "update_protein.to_csv(\"../../data/LPPI/human/protein_updated.csv\", index=False)\n",
    "print(\"Saved: protein_updated.csv\")\n",
    "\n",
    "# === Process LPPI file ===\n",
    "df_lppi = pd.read_csv(lppi_path)\n",
    "\n",
    "# Build protein ID map: p+raw → p+std\n",
    "protein_id_map = {\n",
    "    \"p\" + old: \"p\" + new\n",
    "    for old, new in zip(df_map[\"raw\"], df_map[\"std\"])\n",
    "}\n",
    "\n",
    "# Replace IDs in LPPI where node IDs start with 'p'\n",
    "def replace_protein_id(value):\n",
    "    return protein_id_map.get(value, value) if value.startswith(\"p\") else value\n",
    "\n",
    "df_lppi[\"Node_i\"] = df_lppi[\"Node_i\"].apply(replace_protein_id)\n",
    "df_lppi[\"Node_j\"] = df_lppi[\"Node_j\"].apply(replace_protein_id)\n",
    "\n",
    "# Save the updated LPPI file\n",
    "df_lppi.to_csv(\"../../data/LPPI/human/lppi_updated.csv\", index=False)\n",
    "print(\"Saved: lppi_updated.csv (includes LPI + PPI)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aaeba67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: protein_updated.csv\n",
      "Saved: lppi_updated.csv (includes LPI + PPI)\n"
     ]
    }
   ],
   "source": [
    "# Mouse\n",
    "import pandas as pd\n",
    "\n",
    "# === Configuration file paths ===\n",
    "mapping_path = \"right_mouse_protein.csv\"         \n",
    "protein_path = \"../../data/LPPI/mouse/protein.csv\"        \n",
    "lppi_path = \"../../data/LPPI/mouse/lppi.csv\"               \n",
    "\n",
    "# === Read mapping table ===\n",
    "df_map = pd.read_csv(mapping_path)\n",
    "symbol_map = dict(zip(df_map[\"raw\"], df_map[\"std\"]))\n",
    "\n",
    "# === Process protein file ===\n",
    "df_protein = pd.read_csv(protein_path)\n",
    "df_protein[\"NewSymbol\"] = df_protein[\"protein\"].map(symbol_map).fillna(df_protein[\"protein\"])\n",
    "df_protein[\"NewProteinID\"] = \"p\" + df_protein[\"NewSymbol\"]\n",
    "\n",
    "# Save the updated protein file\n",
    "update_protein = df_protein[[\"NewSymbol\", \"NewProteinID\"]]\n",
    "update_protein.columns = ['protein', 'protein_id']\n",
    "update_protein = update_protein.drop_duplicates()\n",
    "update_protein.to_csv(\"../../data/LPPI/mouse/protein_updated.csv\", index=False)\n",
    "print(\"Saved: protein_updated.csv\")\n",
    "\n",
    "# === Process LPPI file ===\n",
    "df_lppi = pd.read_csv(lppi_path)\n",
    "\n",
    "# Build protein ID map: p+raw → p+std\n",
    "protein_id_map = {\n",
    "    \"p\" + old: \"p\" + new\n",
    "    for old, new in zip(df_map[\"raw\"], df_map[\"std\"])\n",
    "}\n",
    "\n",
    "# Replace IDs in LPPI where node IDs start with 'p'\n",
    "def replace_protein_id(value):\n",
    "    return protein_id_map.get(value, value) if value.startswith(\"p\") else value\n",
    "\n",
    "df_lppi[\"Node_i\"] = df_lppi[\"Node_i\"].apply(replace_protein_id)\n",
    "df_lppi[\"Node_j\"] = df_lppi[\"Node_j\"].apply(replace_protein_id)\n",
    "\n",
    "# Save the updated LPPI file\n",
    "df_lppi.to_csv(\"../../data/LPPI/mouse/lppi_updated.csv\", index=False)\n",
    "print(\"Saved: lppi_updated.csv (includes LPI + PPI)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esslnc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
