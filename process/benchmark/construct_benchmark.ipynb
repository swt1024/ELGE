{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "334f5c3c",
   "metadata": {},
   "source": [
    "### Calculate GIC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b53fce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deduplicating transcripts by sequence: 100%|██████████| 163300/163300 [00:00<00:00, 321733.69it/s]\n",
      "Computing transcript features: 100%|██████████| 154676/154676 [02:13<00:00, 1155.55it/s]\n",
      "Aggregating to lncRNA: 100%|██████████| 35375/35375 [00:01<00:00, 24004.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIC scores have been calculated and saved successfully.\n",
      "Output file is located at: ./human/sorted_GIC_score.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Human GIC scoring with transcript dedup (by identical sequence)\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ======================= paths ======================\n",
    "mapping_file = '../../data/LPI/human/lncRNA_mapping.csv'  # columns: lncRNA_id, member_id (identifier)\n",
    "lnc_trans_path = './human/filtered_lnc_trans.csv'         # columns: identifier, transcript_id\n",
    "MFE_path = './human/trans_MFE.csv'                        # columns: transcript_id, MFE\n",
    "seq_path = './human/transcript_sequences.fasta'           # FASTA for transcripts\n",
    "\n",
    "sorted_GIC_path = './human/sorted_GIC_score.csv'\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "# ---------------- Triplet setup ----------------\n",
    "BASES = ['a', 't', 'c', 'g']\n",
    "TRIPLETS = [a+b+c for a in BASES for b in BASES for c in BASES]\n",
    "mers = {}  # reused container for counting triplets\n",
    "\n",
    "\n",
    "# ---------------- I/O helpers ----------------\n",
    "def read_mapping(filename):\n",
    "    \"\"\"Read lncRNA_id <-> identifier mapping (long format).\"\"\"\n",
    "    df = pd.read_csv(filename, dtype=str)\n",
    "    # build lncRNA_id -> list of identifiers\n",
    "    m = df.groupby('lncRNA_id')['member_id'].agg(list).to_dict()\n",
    "    return m\n",
    "\n",
    "\n",
    "def read_lnc_trans(filename):\n",
    "    \"\"\"Read identifier -> transcript_id mapping (long).\"\"\"\n",
    "    df = pd.read_csv(filename, dtype=str, header=None)\n",
    "    df.columns=['identifier', 'transcript_id']\n",
    "    return df.groupby('identifier')['transcript_id'].agg(list).to_dict()\n",
    "\n",
    "\n",
    "def read_trans_mfe(filename):\n",
    "    \"\"\"Read transcript -> MFE mapping.\"\"\"\n",
    "    df = pd.read_csv(filename, dtype={'transcript_id': str})\n",
    "    return pd.Series(df['MFE'].values, index=df['transcript_id'].astype(str)).to_dict()\n",
    "\n",
    "\n",
    "def read_fasta(filename):\n",
    "    \"\"\"Read transcript FASTA. Return dict: tid -> raw sequence (no newlines).\"\"\"\n",
    "    res = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        tid = None\n",
    "        seq_chunks = []\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                # flush previous\n",
    "                if tid is not None:\n",
    "                    res[tid] = ''.join(seq_chunks)\n",
    "                tid = line.strip()[1:]  # take full header after '>'\n",
    "                seq_chunks = []\n",
    "            else:\n",
    "                seq_chunks.append(line.strip())\n",
    "        if tid is not None:\n",
    "            res[tid] = ''.join(seq_chunks)\n",
    "    return res\n",
    "\n",
    "\n",
    "# -------------- sequence windows & 3-mer freq --------------\n",
    "def slidingWindow(seq, l, win, step=1): \n",
    "    length = l\n",
    "    mod = divmod((length-win), step)[1]\n",
    "    if (win >= length):\n",
    "        return seq\n",
    "    else:\n",
    "        start = 0\n",
    "        end = win\n",
    "        fragments = []\n",
    "        while (len(seq[start:end]) == win):\n",
    "            fragments.append(seq[start:end])\n",
    "            start += step\n",
    "            end += step\n",
    "        if (mod > 0):\n",
    "            fragments.append(seq[(length-win):])\n",
    "        return fragments\n",
    "\n",
    "def stat3mer(seq, l):   \n",
    "    freq = {}\n",
    "    for item in TRIPLETS:\n",
    "        mers[item] = 0\n",
    "    num3mer = float(l-2)\n",
    "    all3mer = slidingWindow(seq, l, win=3, step=1)\n",
    "    for i in set(TRIPLETS):\n",
    "        mers[i] = all3mer.count(i)\n",
    "    for triplet in TRIPLETS:\n",
    "        freq[triplet] = mers[triplet]/num3mer\n",
    "    return freq\n",
    "\n",
    "\n",
    "# -------------- main --------------\n",
    "if __name__ == '__main__':\n",
    "    # 1) load mappings\n",
    "    lnc_to_identifiers = read_mapping(mapping_file)              # lncRNA_id -> [identifier...]\n",
    "    id_to_transcripts = read_lnc_trans(lnc_trans_path)           # identifier -> [transcript_id...]\n",
    "    trans_to_mfe = read_trans_mfe(MFE_path)                      # transcript_id -> MFE\n",
    "    trans_seq_raw = read_fasta(seq_path)                         # transcript_id -> sequence (raw)\n",
    "\n",
    "    # 2) transcript dedup by identical sequence (normalize to lowercase, strip N/LF)\n",
    "    seq_to_rep = {}                  # normalized_seq -> representative transcript_id\n",
    "    rep_seq = {}                     # representative transcript_id -> normalized_seq\n",
    "    trans_to_rep = {}                # transcript_id -> representative transcript_id\n",
    "\n",
    "    for tid, raw in tqdm(trans_seq_raw.items(), desc=\"Deduplicating transcripts by sequence\"):\n",
    "        norm = raw.replace('\\n', '').replace('\\r', '').lower()\n",
    "        if norm not in seq_to_rep:\n",
    "            seq_to_rep[norm] = tid\n",
    "            rep_seq[tid] = norm\n",
    "        trans_to_rep[tid] = seq_to_rep[norm]\n",
    "\n",
    "    # 3) compute features for representative transcripts only (those that have MFE)\n",
    "    LRMODEL_FEATURES_7 = ['intercept', 'length', 'mfe/L', 'cga', 'gcg', 'tcg', 'acg', 'tca']\n",
    "    LRMODEL_COEFS_7 = [0.7417, 2.612e-04, 4.295, 48.66, 15.64, 76.23, -1.113, -60.29]\n",
    "    LRMODEL_7 = dict(zip(LRMODEL_FEATURES_7, LRMODEL_COEFS_7))\n",
    "\n",
    "    features_trans = {}  # rep_tid -> feature dict\n",
    "    for rep_tid, norm_seq in tqdm(rep_seq.items(), desc=\"Computing transcript features\"):\n",
    "        # Use MFE of the representative if available; if not, try to borrow from any duplicate with same seq\n",
    "        mfe_val = trans_to_mfe.get(rep_tid, None)\n",
    "        if mfe_val is None:\n",
    "            # search any transcript that maps to this representative and has MFE\n",
    "            # (rare case: choose the first available MFE among duplicates)\n",
    "            for tid, r in trans_to_rep.items():\n",
    "                if r == rep_tid and tid in trans_to_mfe:\n",
    "                    mfe_val = trans_to_mfe[tid]\n",
    "                    break\n",
    "        if mfe_val is None:\n",
    "            # cannot compute without MFE\n",
    "            continue\n",
    "\n",
    "        L = len(norm_seq)\n",
    "        f = {'trans': rep_tid, 'length': L, 'mfe/L': float(mfe_val) / L if L > 0 else 0.0}\n",
    "        freq = stat3mer(norm_seq,L)\n",
    "        for t in TRIPLETS:\n",
    "            f[t] = freq[t]\n",
    "        features_trans[rep_tid] = f\n",
    "\n",
    "    # 4) aggregate to lncRNA level (dedup transcripts by sequence within each lncRNA)\n",
    "    lncRNA_GIC_score = {}\n",
    "    for lnc_id, identifiers in tqdm(lnc_to_identifiers.items(), desc=\"Aggregating to lncRNA\"):\n",
    "        # collect all transcripts from all identifiers\n",
    "        all_transcripts = []\n",
    "        for ident in identifiers:\n",
    "            all_transcripts.extend(id_to_transcripts.get(ident, []))\n",
    "\n",
    "        if not all_transcripts:\n",
    "            continue\n",
    "\n",
    "        # map to representative (sequence dedup) and keep unique reps\n",
    "        rep_set = {trans_to_rep.get(tid, None) for tid in all_transcripts}\n",
    "        rep_set.discard(None)\n",
    "\n",
    "        # keep only reps with computed features (i.e., have sequence + MFE)\n",
    "        usable_reps = [r for r in rep_set if r in features_trans]\n",
    "        if not usable_reps:\n",
    "            continue\n",
    "\n",
    "        # average features across usable representative transcripts\n",
    "        agg = {'length': 0.0, 'mfe/L': 0.0}\n",
    "        for t in TRIPLETS:\n",
    "            agg[t] = 0.0\n",
    "\n",
    "        for rep_tid in usable_reps:\n",
    "            trf = features_trans[rep_tid]\n",
    "            agg['length'] += trf['length']\n",
    "            agg['mfe/L'] += trf['mfe/L']\n",
    "            for t in TRIPLETS:\n",
    "                agg[t] += trf[t]\n",
    "\n",
    "        n = float(len(usable_reps))\n",
    "        agg['length'] /= n\n",
    "        agg['mfe/L'] /= n\n",
    "        for t in TRIPLETS:\n",
    "            agg[t] /= n\n",
    "\n",
    "        # logistic regression (7-feature model)\n",
    "        tmp = (LRMODEL_7['intercept']\n",
    "               + LRMODEL_7['length'] * agg['length']\n",
    "               + LRMODEL_7['mfe/L'] * agg['mfe/L']\n",
    "               + LRMODEL_7['cga'] * agg['cga']\n",
    "               + LRMODEL_7['gcg'] * agg['gcg']\n",
    "               + LRMODEL_7['tcg'] * agg['tcg']\n",
    "               + LRMODEL_7['acg'] * agg['acg']\n",
    "               + LRMODEL_7['tca'] * agg['tca'])\n",
    "        gic = math.exp(tmp) / (math.exp(tmp) + 1.0)\n",
    "        lncRNA_GIC_score[lnc_id] = gic\n",
    "\n",
    "    # 5) export sorted scores\n",
    "    lncRNA_scores_df_sorted = (pd.Series(lncRNA_GIC_score, name='GIC_score')\n",
    "                                 .rename_axis('lncRNA_id')\n",
    "                                 .reset_index()\n",
    "                                 .sort_values('GIC_score', ascending=False))\n",
    "    lncRNA_scores_df_sorted = lncRNA_scores_df_sorted.dropna(subset=[\"GIC_score\"])\n",
    "    lncRNA_scores_df_sorted.to_csv(sorted_GIC_path, index=False)\n",
    "\n",
    "    print(\"GIC scores have been calculated and saved successfully.\")\n",
    "    print(f\"Output file is located at: {sorted_GIC_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab19220c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deduplicating transcripts by sequence: 100%|██████████| 56380/56380 [00:00<00:00, 362299.75it/s]\n",
      "Computing transcript features: 100%|██████████| 52895/52895 [01:07<00:00, 782.14it/s] \n",
      "Aggregating to lncRNA: 100%|██████████| 29025/29025 [00:00<00:00, 42749.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIC scores have been calculated and saved successfully.\n",
      "Output file is located at: ./mouse/sorted_GIC_score.csv\n"
     ]
    }
   ],
   "source": [
    "# Mouse GIC scoring with transcript dedup (by identical sequence)\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ======================= paths ======================\n",
    "mapping_file = '../../data/LPI/mouse/lncRNA_mapping.csv'  # columns: lncRNA_id, member_id (identifier)\n",
    "lnc_trans_path = './mouse/filtered_lnc_trans.csv'         # columns: identifier, transcript_id\n",
    "MFE_path = './mouse/trans_MFE.csv'                        # columns: transcript_id, MFE\n",
    "seq_path = './mouse/transcript_sequences.fasta'           # FASTA for transcripts\n",
    "\n",
    "sorted_GIC_path = './mouse/sorted_GIC_score.csv'\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "# ---------------- Triplet setup ----------------\n",
    "BASES = ['a', 't', 'c', 'g']\n",
    "TRIPLETS = [a+b+c for a in BASES for b in BASES for c in BASES]\n",
    "mers = {}  # reused container for counting triplets\n",
    "\n",
    "\n",
    "# ---------------- I/O helpers ----------------\n",
    "def read_mapping(filename):\n",
    "    \"\"\"Read lncRNA_id <-> identifier mapping (long format).\"\"\"\n",
    "    df = pd.read_csv(filename, dtype=str)\n",
    "    # build lncRNA_id -> list of identifiers\n",
    "    m = df.groupby('lncRNA_id')['member_id'].agg(list).to_dict()\n",
    "    return m\n",
    "\n",
    "\n",
    "def read_lnc_trans(filename):\n",
    "    \"\"\"Read identifier -> transcript_id mapping (long).\"\"\"\n",
    "    df = pd.read_csv(filename, dtype=str, header=None)\n",
    "    df.columns=['identifier', 'transcript_id']\n",
    "    return df.groupby('identifier')['transcript_id'].agg(list).to_dict()\n",
    "\n",
    "\n",
    "def read_trans_mfe(filename):\n",
    "    \"\"\"Read transcript -> MFE mapping.\"\"\"\n",
    "    df = pd.read_csv(filename, dtype={'transcript_id': str})\n",
    "    return pd.Series(df['MFE'].values, index=df['transcript_id'].astype(str)).to_dict()\n",
    "\n",
    "\n",
    "def read_fasta(filename):\n",
    "    \"\"\"Read transcript FASTA. Return dict: tid -> raw sequence (no newlines).\"\"\"\n",
    "    res = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        tid = None\n",
    "        seq_chunks = []\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                # flush previous\n",
    "                if tid is not None:\n",
    "                    res[tid] = ''.join(seq_chunks)\n",
    "                tid = line.strip()[1:]  # take full header after '>'\n",
    "                seq_chunks = []\n",
    "            else:\n",
    "                seq_chunks.append(line.strip())\n",
    "        if tid is not None:\n",
    "            res[tid] = ''.join(seq_chunks)\n",
    "    return res\n",
    "\n",
    "\n",
    "# -------------- sequence windows & 3-mer freq --------------\n",
    "def slidingWindow(seq, l, win, step=1): \n",
    "    length = l\n",
    "    mod = divmod((length-win), step)[1]\n",
    "    if (win >= length):\n",
    "        return seq\n",
    "    else:\n",
    "        start = 0\n",
    "        end = win\n",
    "        fragments = []\n",
    "        while (len(seq[start:end]) == win):\n",
    "            fragments.append(seq[start:end])\n",
    "            start += step\n",
    "            end += step\n",
    "        if (mod > 0):\n",
    "            fragments.append(seq[(length-win):])\n",
    "        return fragments\n",
    "\n",
    "def stat3mer(seq, l):   \n",
    "    freq = {}\n",
    "    for item in TRIPLETS:\n",
    "        mers[item] = 0\n",
    "    num3mer = float(l-2)\n",
    "    all3mer = slidingWindow(seq, l, win=3, step=1)\n",
    "    for i in set(TRIPLETS):\n",
    "        mers[i] = all3mer.count(i)\n",
    "    for triplet in TRIPLETS:\n",
    "        freq[triplet] = mers[triplet]/num3mer\n",
    "    return freq\n",
    "\n",
    "\n",
    "# -------------- main --------------\n",
    "if __name__ == '__main__':\n",
    "    # 1) load mappings\n",
    "    lnc_to_identifiers = read_mapping(mapping_file)              # lncRNA_id -> [identifier...]\n",
    "    id_to_transcripts = read_lnc_trans(lnc_trans_path)           # identifier -> [transcript_id...]\n",
    "    trans_to_mfe = read_trans_mfe(MFE_path)                      # transcript_id -> MFE\n",
    "    trans_seq_raw = read_fasta(seq_path)                         # transcript_id -> sequence (raw)\n",
    "\n",
    "    # 2) transcript dedup by identical sequence (normalize to lowercase, strip N/LF)\n",
    "    seq_to_rep = {}                  # normalized_seq -> representative transcript_id\n",
    "    rep_seq = {}                     # representative transcript_id -> normalized_seq\n",
    "    trans_to_rep = {}                # transcript_id -> representative transcript_id\n",
    "\n",
    "    for tid, raw in tqdm(trans_seq_raw.items(), desc=\"Deduplicating transcripts by sequence\"):\n",
    "        norm = raw.replace('\\n', '').replace('\\r', '').lower()\n",
    "        if norm not in seq_to_rep:\n",
    "            seq_to_rep[norm] = tid\n",
    "            rep_seq[tid] = norm\n",
    "        trans_to_rep[tid] = seq_to_rep[norm]\n",
    "\n",
    "    # 3) compute features for representative transcripts only (those that have MFE)\n",
    "    LRMODEL_FEATURES_7 = ['intercept', 'length', 'mfe/L', 'cga', 'gcg', 'tcg', 'acg', 'tca']\n",
    "    LRMODEL_COEFS_7 = [0.7417, 2.612e-04, 4.295, 48.66, 15.64, 76.23, -1.113, -60.29]\n",
    "    LRMODEL_7 = dict(zip(LRMODEL_FEATURES_7, LRMODEL_COEFS_7))\n",
    "\n",
    "    features_trans = {}  # rep_tid -> feature dict\n",
    "    for rep_tid, norm_seq in tqdm(rep_seq.items(), desc=\"Computing transcript features\"):\n",
    "        # Use MFE of the representative if available; if not, try to borrow from any duplicate with same seq\n",
    "        mfe_val = trans_to_mfe.get(rep_tid, None)\n",
    "        if mfe_val is None:\n",
    "            # search any transcript that maps to this representative and has MFE\n",
    "            # (rare case: choose the first available MFE among duplicates)\n",
    "            for tid, r in trans_to_rep.items():\n",
    "                if r == rep_tid and tid in trans_to_mfe:\n",
    "                    mfe_val = trans_to_mfe[tid]\n",
    "                    break\n",
    "        if mfe_val is None:\n",
    "            # cannot compute without MFE\n",
    "            continue\n",
    "\n",
    "        L = len(norm_seq)\n",
    "        f = {'trans': rep_tid, 'length': L, 'mfe/L': float(mfe_val) / L if L > 0 else 0.0}\n",
    "        freq = stat3mer(norm_seq,L)\n",
    "        for t in TRIPLETS:\n",
    "            f[t] = freq[t]\n",
    "        features_trans[rep_tid] = f\n",
    "\n",
    "    # 4) aggregate to lncRNA level (dedup transcripts by sequence within each lncRNA)\n",
    "    lncRNA_GIC_score = {}\n",
    "    for lnc_id, identifiers in tqdm(lnc_to_identifiers.items(), desc=\"Aggregating to lncRNA\"):\n",
    "        # collect all transcripts from all identifiers\n",
    "        all_transcripts = []\n",
    "        for ident in identifiers:\n",
    "            all_transcripts.extend(id_to_transcripts.get(ident, []))\n",
    "\n",
    "        if not all_transcripts:\n",
    "            continue\n",
    "\n",
    "        # map to representative (sequence dedup) and keep unique reps\n",
    "        rep_set = {trans_to_rep.get(tid, None) for tid in all_transcripts}\n",
    "        rep_set.discard(None)\n",
    "\n",
    "        # keep only reps with computed features (i.e., have sequence + MFE)\n",
    "        usable_reps = [r for r in rep_set if r in features_trans]\n",
    "        if not usable_reps:\n",
    "            continue\n",
    "\n",
    "        # average features across usable representative transcripts\n",
    "        agg = {'length': 0.0, 'mfe/L': 0.0}\n",
    "        for t in TRIPLETS:\n",
    "            agg[t] = 0.0\n",
    "\n",
    "        for rep_tid in usable_reps:\n",
    "            trf = features_trans[rep_tid]\n",
    "            agg['length'] += trf['length']\n",
    "            agg['mfe/L'] += trf['mfe/L']\n",
    "            for t in TRIPLETS:\n",
    "                agg[t] += trf[t]\n",
    "\n",
    "        n = float(len(usable_reps))\n",
    "        agg['length'] /= n\n",
    "        agg['mfe/L'] /= n\n",
    "        for t in TRIPLETS:\n",
    "            agg[t] /= n\n",
    "\n",
    "        # logistic regression (7-feature model)\n",
    "        tmp = (LRMODEL_7['intercept']\n",
    "               + LRMODEL_7['length'] * agg['length']\n",
    "               + LRMODEL_7['mfe/L'] * agg['mfe/L']\n",
    "               + LRMODEL_7['cga'] * agg['cga']\n",
    "               + LRMODEL_7['gcg'] * agg['gcg']\n",
    "               + LRMODEL_7['tcg'] * agg['tcg']\n",
    "               + LRMODEL_7['acg'] * agg['acg']\n",
    "               + LRMODEL_7['tca'] * agg['tca'])\n",
    "        gic = math.exp(tmp) / (math.exp(tmp) + 1.0)\n",
    "        lncRNA_GIC_score[lnc_id] = gic\n",
    "\n",
    "    # 5) export sorted scores\n",
    "    lncRNA_scores_df_sorted = (pd.Series(lncRNA_GIC_score, name='GIC_score')\n",
    "                                 .rename_axis('lncRNA_id')\n",
    "                                 .reset_index()\n",
    "                                 .sort_values('GIC_score', ascending=False))\n",
    "    lncRNA_scores_df_sorted.to_csv(sorted_GIC_path, index=False)\n",
    "\n",
    "    print(\"GIC scores have been calculated and saved successfully.\")\n",
    "    print(f\"Output file is located at: {sorted_GIC_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdca003",
   "metadata": {},
   "source": [
    "### Get essential lncRNA samples and non-essential lncRNA samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39b00b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "esslnc = pd.read_csv(\"../../data/raw/esslnc.csv\")\n",
    "\n",
    "esslnc = esslnc[[\"Noncode_id\",\"target\",\"gene_name\",\"ensembl_id\",\"Organism\",\"cancer_related\",\"disease_related\",\"vivo\",\"vitro\"]]\n",
    "\n",
    "human_esslnc = esslnc[esslnc['Organism']=='Human']\n",
    "mouse_esslnc = esslnc[esslnc['Organism']=='Mouse']\n",
    "\n",
    "human_esslnc.to_csv('../../data/benchmark/human/human_esslnc.csv',index=False)\n",
    "mouse_esslnc.to_csv('../../data/benchmark/mouse/mouse_esslnc.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5138a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human\n",
    "import pandas as pd\n",
    "\n",
    "# Read the lncRNA data\n",
    "ess_lnc = pd.read_csv(\"../../data/benchmark/human/human_esslnc.csv\")\n",
    "lncRNA = pd.read_csv(\"../../data/LPI/human/lncRNA.csv\")\n",
    "lncRNA_mapping = pd.read_csv(\"../../data/LPI/human/lncRNA_mapping.csv\")\n",
    "annotation = pd.read_csv(\"../../annotate/human/valid_heart_annotation.csv\")\n",
    "\n",
    "ess_lnc['Noncode_id'] = ess_lnc['Noncode_id'].str.split('.').str[0]\n",
    "def is_essential(id, name):\n",
    "    if id != '-' and any(ess_lnc[col].isin([id]).any() for col in ['Noncode_id', 'ensembl_id']):\n",
    "        return 1\n",
    "    if name != '-' and any(ess_lnc[col].isin([name]).any() for col in ['gene_name', 'target']):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "lncRNA.loc[:, 'essential'] = lncRNA.apply(lambda row: is_essential(row['gene_id'], row['gene_name']), axis=1)\n",
    "ess_lnc = lncRNA[lncRNA['essential'] == 1]\n",
    "\n",
    "ess_lnc_id = lncRNA_mapping[lncRNA_mapping['member_id'].isin(ess_lnc['identifier'])].copy()\n",
    "anno_ess_lnc = ess_lnc_id[ess_lnc_id['lncRNA_id'].isin(annotation['lncRNA_id'])]\n",
    "anno_ess_lnc = anno_ess_lnc[['lncRNA_id']].drop_duplicates()\n",
    "anno_ess_lnc.to_csv('../../data/benchmark/human/ess_lnc.csv', index=False)\n",
    "\n",
    "lnc_GIC = pd.read_csv(\"./human/sorted_GIC_score.csv\")\n",
    "\n",
    "unlabel_lnc = lnc_GIC[~lnc_GIC['lncRNA_id'].isin(anno_ess_lnc['lncRNA_id'])]\n",
    "unlabel_lnc = unlabel_lnc[unlabel_lnc['lncRNA_id'].isin(annotation['lncRNA_id'])]\n",
    "\n",
    "ess_counts = anno_ess_lnc.shape[0]\n",
    "\n",
    "noness_lnc = unlabel_lnc.tail(ess_counts)[['lncRNA_id']].reset_index(drop=True)\n",
    "\n",
    "noness_lnc.to_csv(\"../../data/benchmark/human/noness_lnc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f37be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mouse\n",
    "import pandas as pd\n",
    "\n",
    "# Read the lncRNA data\n",
    "ess_lnc = pd.read_csv(\"../../data/benchmark/mouse/mouse_esslnc.csv\")\n",
    "lncRNA = pd.read_csv(\"../../data/LPI/mouse/lncRNA.csv\")\n",
    "lncRNA_mapping = pd.read_csv(\"../../data/LPI/mouse/lncRNA_mapping.csv\")\n",
    "annotation = pd.read_csv(\"../../annotate/mouse/valid_heart_annotation.csv\")\n",
    "\n",
    "ess_lnc['Noncode_id'] = ess_lnc['Noncode_id'].str.split('.').str[0]\n",
    "def is_essential(id, name):\n",
    "    if id != '-' and any(ess_lnc[col].isin([id]).any() for col in ['Noncode_id', 'ensembl_id']):\n",
    "        return 1\n",
    "    if name != '-' and any(ess_lnc[col].isin([name]).any() for col in ['gene_name', 'target']):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "lncRNA.loc[:, 'essential'] = lncRNA.apply(lambda row: is_essential(row['gene_id'], row['gene_name']), axis=1)\n",
    "ess_lnc = lncRNA[lncRNA['essential'] == 1]\n",
    "\n",
    "ess_lnc_id = lncRNA_mapping[lncRNA_mapping['member_id'].isin(ess_lnc['identifier'])].copy()\n",
    "anno_ess_lnc = ess_lnc_id[ess_lnc_id['lncRNA_id'].isin(annotation['lncRNA_id'])]\n",
    "anno_ess_lnc = anno_ess_lnc[['lncRNA_id']].drop_duplicates()\n",
    "anno_ess_lnc.to_csv('../../data/benchmark/mouse/ess_lnc.csv', index=False)\n",
    "\n",
    "lnc_GIC = pd.read_csv(\"./mouse/sorted_GIC_score.csv\")\n",
    "\n",
    "unlabel_lnc = lnc_GIC[~lnc_GIC['lncRNA_id'].isin(anno_ess_lnc['lncRNA_id'])]\n",
    "unlabel_lnc = unlabel_lnc[unlabel_lnc['lncRNA_id'].isin(annotation['lncRNA_id'])]\n",
    "\n",
    "ess_counts = anno_ess_lnc.shape[0]\n",
    "\n",
    "noness_lnc = unlabel_lnc.tail(ess_counts)[['lncRNA_id']].reset_index(drop=True)\n",
    "\n",
    "noness_lnc.to_csv(\"../../data/benchmark/mouse/noness_lnc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c245c5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Matching identifiers...\n",
      "Aggregating nodes...\n",
      "Formatting output table...\n",
      "Success! Generated ../../data/benchmark/human/Supplementary_Table_S8.xlsx\n",
      "Total Essential Nodes Identified: 1372\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Loading\n",
    "# ==========================================\n",
    "print(\"Loading data...\")\n",
    "# Load the essential lncRNA database (Ground Truth)\n",
    "ess_lnc_db = pd.read_csv(\"../../data/benchmark/human/human_esslnc.csv\")\n",
    "\n",
    "# Load raw lncRNA member information (contains gene_id, gene_name)\n",
    "lncRNA = pd.read_csv(\"../../data/LPI/human/lncRNA.csv\") \n",
    "\n",
    "# Load mapping file: Member ID -> Unified Node ID (lncRNA_id)\n",
    "lncRNA_mapping = pd.read_csv(\"../../data/LPI/human/lncRNA_mapping.csv\") \n",
    "\n",
    "annotation = pd.read_csv(\"../../annotate/human/valid_heart_annotation.csv\") \n",
    "\n",
    "bed = pd.read_csv(\"../../data/LPI/human/lncRNA_dedup.bed\", header=None, sep='\\t')\n",
    "bed.columns=['chr','start','end','lncRNA_id','score','strand']\n",
    "# ==========================================\n",
    "# 2. Build Hash Set for Fast Lookup\n",
    "# ==========================================\n",
    "# Clean version numbers in Noncode_id (remove suffixes like .1)\n",
    "ess_lnc_db['Noncode_id'] = ess_lnc_db['Noncode_id'].str.split('.').str[0]\n",
    "\n",
    "# Collect all potential essential identifiers into a set for O(1) lookup speed\n",
    "valid_ess_ids = set(ess_lnc_db['Noncode_id'].dropna()) | \\\n",
    "                    set(ess_lnc_db['ensembl_id'].dropna()) \n",
    "\n",
    "valid_ess_names = set(ess_lnc_db['gene_name'].dropna()) | \\\n",
    "                    set(ess_lnc_db['target'].dropna())\n",
    "\n",
    "# ==========================================\n",
    "# 3. Identify Evidence (Member Level)\n",
    "# ==========================================\n",
    "print(\"Matching identifiers...\")\n",
    "\n",
    "# Function to return the specific matching ID as evidence\n",
    "def get_evidence(row):\n",
    "    # Check gene_id first\n",
    "    if row['gene_id'] in valid_ess_ids:\n",
    "        return row['gene_id']\n",
    "    # Check gene_name\n",
    "    if row['gene_name'] in valid_ess_names:\n",
    "        return row['gene_name']\n",
    "    return None\n",
    "\n",
    "# Merge raw member info with the Unified Node ID mapping\n",
    "# Assumption: lncRNA['identifier'] corresponds to lncRNA_mapping['member_id']\n",
    "merged_df = lncRNA.merge(lncRNA_mapping, left_on='identifier', right_on='member_id', how='inner')\n",
    "\n",
    "# Apply the evidence check to every row (constituent member)\n",
    "merged_df['match_evidence'] = merged_df.apply(get_evidence, axis=1)\n",
    "\n",
    "# ==========================================\n",
    "# 4. Aggregation (Node Level)\n",
    "# ==========================================\n",
    "print(\"Aggregating nodes...\")\n",
    "\n",
    "# Helper function: Remove duplicates, sort, join with semicolons, and exclude empty values\n",
    "def join_unique(series):\n",
    "    # Filter out NaNs and placeholders like '-', then sort and join\n",
    "    items = sorted(set([str(i) for i in series.dropna() if str(i) != '-']))\n",
    "    return ';'.join(items)\n",
    "\n",
    "def join_evidence(series):\n",
    "    # Filter out None/NaNs for the evidence column\n",
    "    items = sorted(set([str(i) for i in series.dropna()]))\n",
    "    return ';'.join(items)\n",
    "\n",
    "# Group by Unified Node ID (lncRNA_id) and aggregate attributes\n",
    "grouped_nodes = merged_df.groupby('lncRNA_id').agg({\n",
    "    'gene_id': join_unique,       # Combine all constituent IDs\n",
    "    'gene_name': join_unique,     # Combine all constituent symbols\n",
    "    'match_evidence': join_evidence # Combine all matched evidence IDs\n",
    "}).reset_index()\n",
    "\n",
    "# Filter for essential nodes (where evidence is not empty)\n",
    "ess_nodes_final = grouped_nodes[grouped_nodes['match_evidence'] != ''].copy()\n",
    "ess_nodes_final = ess_nodes_final[ess_nodes_final['lncRNA_id'].isin(annotation['lncRNA_id'])]\n",
    "\n",
    "# ==========================================\n",
    "# 5. Format Final Table\n",
    "# ==========================================\n",
    "print(\"Formatting output table...\")\n",
    "\n",
    "# Format genomic coordinates as \"chr:start-end:strand\"\n",
    "# Ensure the annotation dataframe has columns: chromosome, start, end, strand\n",
    "bed['Genomic_Coordinates'] = bed.apply(\n",
    "    lambda x: f\"{x['chr']}:{x['start']}-{x['end']}:{x['strand']}\", axis=1\n",
    ")\n",
    "\n",
    "# Merge coordinate information into the final essential list\n",
    "final_table = ess_nodes_final.merge(bed[['lncRNA_id', 'Genomic_Coordinates']], on='lncRNA_id', how='inner')\n",
    "\n",
    "# Rename columns to match the manuscript's Supplementary Table terminology\n",
    "final_table = final_table.rename(columns={\n",
    "    'lncRNA_id': 'Unified_Node_ID',\n",
    "    'gene_id': 'Constituent_Gene_IDs',\n",
    "    'gene_name': 'Constituent_Gene_Symbols',\n",
    "    'match_evidence': 'Matched_Evidence_ID'\n",
    "})\n",
    "\n",
    "# Reorder columns for clarity\n",
    "final_table = final_table[[\n",
    "    'Unified_Node_ID', \n",
    "    'Genomic_Coordinates', \n",
    "    'Constituent_Gene_IDs', \n",
    "    'Constituent_Gene_Symbols', \n",
    "    'Matched_Evidence_ID'\n",
    "]]\n",
    "\n",
    "# ==========================================\n",
    "# 6. Export to Excel\n",
    "# ==========================================\n",
    "output_path = \"../../data/benchmark/human/Supplementary_Table_S8.xlsx\"\n",
    "final_table.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Success! Generated {output_path}\")\n",
    "print(f\"Total Essential Nodes Identified: {len(final_table)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ef7c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Matching identifiers...\n",
      "Aggregating nodes...\n",
      "Formatting output table...\n",
      "Success! Generated ../../data/benchmark/mouse/Supplementary_Table_S9.xlsx\n",
      "Total Essential Nodes Identified: 24\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Loading\n",
    "# ==========================================\n",
    "print(\"Loading data...\")\n",
    "# Load the essential lncRNA database (Ground Truth)\n",
    "ess_lnc_db = pd.read_csv(\"../../data/benchmark/mouse/mouse_esslnc.csv\")\n",
    "\n",
    "# Load raw lncRNA member information (contains gene_id, gene_name)\n",
    "lncRNA = pd.read_csv(\"../../data/LPI/mouse/lncRNA.csv\") \n",
    "\n",
    "# Load mapping file: Member ID -> Unified Node ID (lncRNA_id)\n",
    "lncRNA_mapping = pd.read_csv(\"../../data/LPI/mouse/lncRNA_mapping.csv\") \n",
    "\n",
    "annotation = pd.read_csv(\"../../annotate/mouse/valid_heart_annotation.csv\") \n",
    "\n",
    "bed = pd.read_csv(\"../../data/LPI/mouse/lncRNA_dedup.bed\", header=None, sep='\\t')\n",
    "bed.columns=['chr','start','end','lncRNA_id','score','strand']\n",
    "# ==========================================\n",
    "# 2. Build Hash Set for Fast Lookup\n",
    "# ==========================================\n",
    "# Clean version numbers in Noncode_id (remove suffixes like .1)\n",
    "ess_lnc_db['Noncode_id'] = ess_lnc_db['Noncode_id'].str.split('.').str[0]\n",
    "\n",
    "# Collect all potential essential identifiers into a set for O(1) lookup speed\n",
    "valid_ess_ids = set(ess_lnc_db['Noncode_id'].dropna()) | \\\n",
    "                    set(ess_lnc_db['ensembl_id'].dropna()) \n",
    "\n",
    "valid_ess_names = set(ess_lnc_db['gene_name'].dropna()) | \\\n",
    "                    set(ess_lnc_db['target'].dropna())\n",
    "\n",
    "# ==========================================\n",
    "# 3. Identify Evidence (Member Level)\n",
    "# ==========================================\n",
    "print(\"Matching identifiers...\")\n",
    "\n",
    "# Function to return the specific matching ID as evidence\n",
    "def get_evidence(row):\n",
    "    # Check gene_id first\n",
    "    if row['gene_id'] in valid_ess_ids:\n",
    "        return row['gene_id']\n",
    "    # Check gene_name\n",
    "    if row['gene_name'] in valid_ess_names:\n",
    "        return row['gene_name']\n",
    "    return None\n",
    "\n",
    "# Merge raw member info with the Unified Node ID mapping\n",
    "# Assumption: lncRNA['identifier'] corresponds to lncRNA_mapping['member_id']\n",
    "merged_df = lncRNA.merge(lncRNA_mapping, left_on='identifier', right_on='member_id', how='inner')\n",
    "\n",
    "# Apply the evidence check to every row (constituent member)\n",
    "merged_df['match_evidence'] = merged_df.apply(get_evidence, axis=1)\n",
    "\n",
    "# ==========================================\n",
    "# 4. Aggregation (Node Level)\n",
    "# ==========================================\n",
    "print(\"Aggregating nodes...\")\n",
    "\n",
    "# Helper function: Remove duplicates, sort, join with semicolons, and exclude empty values\n",
    "def join_unique(series):\n",
    "    # Filter out NaNs and placeholders like '-', then sort and join\n",
    "    items = sorted(set([str(i) for i in series.dropna() if str(i) != '-']))\n",
    "    return ';'.join(items)\n",
    "\n",
    "def join_evidence(series):\n",
    "    # Filter out None/NaNs for the evidence column\n",
    "    items = sorted(set([str(i) for i in series.dropna()]))\n",
    "    return ';'.join(items)\n",
    "\n",
    "# Group by Unified Node ID (lncRNA_id) and aggregate attributes\n",
    "grouped_nodes = merged_df.groupby('lncRNA_id').agg({\n",
    "    'gene_id': join_unique,       # Combine all constituent IDs\n",
    "    'gene_name': join_unique,     # Combine all constituent symbols\n",
    "    'match_evidence': join_evidence # Combine all matched evidence IDs\n",
    "}).reset_index()\n",
    "\n",
    "# Filter for essential nodes (where evidence is not empty)\n",
    "ess_nodes_final = grouped_nodes[grouped_nodes['match_evidence'] != ''].copy()\n",
    "ess_nodes_final = ess_nodes_final[ess_nodes_final['lncRNA_id'].isin(annotation['lncRNA_id'])]\n",
    "\n",
    "# ==========================================\n",
    "# 5. Format Final Table\n",
    "# ==========================================\n",
    "print(\"Formatting output table...\")\n",
    "\n",
    "# Format genomic coordinates as \"chr:start-end:strand\"\n",
    "# Ensure the annotation dataframe has columns: chromosome, start, end, strand\n",
    "bed['Genomic_Coordinates'] = bed.apply(\n",
    "    lambda x: f\"{x['chr']}:{x['start']}-{x['end']}:{x['strand']}\", axis=1\n",
    ")\n",
    "\n",
    "# Merge coordinate information into the final essential list\n",
    "final_table = ess_nodes_final.merge(bed[['lncRNA_id', 'Genomic_Coordinates']], on='lncRNA_id', how='inner')\n",
    "\n",
    "# Rename columns to match the manuscript's Supplementary Table terminology\n",
    "final_table = final_table.rename(columns={\n",
    "    'lncRNA_id': 'Unified_Node_ID',\n",
    "    'gene_id': 'Constituent_Gene_IDs',\n",
    "    'gene_name': 'Constituent_Gene_Symbols',\n",
    "    'match_evidence': 'Matched_Evidence_ID'\n",
    "})\n",
    "\n",
    "# Reorder columns for clarity\n",
    "final_table = final_table[[\n",
    "    'Unified_Node_ID', \n",
    "    'Genomic_Coordinates', \n",
    "    'Constituent_Gene_IDs', \n",
    "    'Constituent_Gene_Symbols', \n",
    "    'Matched_Evidence_ID'\n",
    "]]\n",
    "\n",
    "# ==========================================\n",
    "# 6. Export to Excel\n",
    "# ==========================================\n",
    "output_path = \"../../data/benchmark/mouse/Supplementary_Table_S9.xlsx\"\n",
    "final_table.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Success! Generated {output_path}\")\n",
    "print(f\"Total Essential Nodes Identified: {len(final_table)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ELE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
